search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202504182000+TO+202504242000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, cs.AI, stat.*, physics.data-an staritng 202504182000 and ending 202504242000</h1>Feed last updated: 2025-04-24T00:00:00-04:00<a href="http://arxiv.org/pdf/2504.16079v1"><h2>A Distribution-Free Approach to Testing Models for Angular Power Spectra</h2></a>Authors:  Sara Algeri, Xiangyu Zhang, Erik Floden, Hongru Zhao, Galin L. Jones, Vuk Mandic, Jesse Miller</br>Comments: No comment found</br>Primary Category: physics.data-an</br>All Categories: physics.data-an, astro-ph.IM</br><p>A novel goodness-of-fit strategy is introduced for testing models for angular
power spectra characterized by unknown parameters. Using this strategy, it is
possible to assess the validity of such models without specifying the
distribution of the estimators of the angular power spectrum being used. This
holds under general conditions, ensuring the applicability of the method across
diverse scenarios. Moreover, the proposed solution overcomes the need for
case-by-case simulations when testing different models -- leading to notable
computational advantages.</p></br><a href="http://arxiv.org/pdf/2504.15679v1"><h2>Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE
  Problem using Soft Actor-Critic Reinforcement Learning</h2></a>Authors:  Brandon Panos, Ivan Milic</br>Comments: No comment found</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, cs.LG</br><p>We present a novel reinforcement learning (RL) approach for solving the
classical 2-level atom non-LTE radiative transfer problem by framing it as a
control task in which an RL agent learns a depth-dependent source function
$S(\tau)$ that self-consistently satisfies the equation of statistical
equilibrium (SE). The agent's policy is optimized entirely via reward-based
interactions with a radiative transfer engine, without explicit knowledge of
the ground truth. This method bypasses the need for constructing approximate
lambda operators ($\Lambda^*$) common in accelerated iterative schemes.
Additionally, it requires no extensive precomputed labeled datasets to extract
a supervisory signal, and avoids backpropagating gradients through the complex
RT solver itself. Finally, we show through experiment that a simple feedforward
neural network trained greedily cannot solve for SE, possibly due to the moving
target nature of the problem. Our $\Lambda^*-\text{Free}$ method offers
potential advantages for complex scenarios (e.g., atmospheres with enhanced
velocity fields, multi-dimensional geometries, or complex microphysics) where
$\Lambda^*$ construction or solver differentiability is challenging.
Additionally, the agent can be incentivized to find more efficient policies by
manipulating the discount factor, leading to a reprioritization of immediate
rewards. If demonstrated to generalize past its training data, this RL
framework could serve as an alternative or accelerated formalism to achieve SE.
To the best of our knowledge, this study represents the first application of
reinforcement learning in solar physics that directly solves for a fundamental
physical constraint.</p></br>
