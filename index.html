search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202409142000+TO+202409202000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on physics.data-an, cs.LG, stat.*, cs.AI staritng 202409142000 and ending 202409202000</h1>Feed last updated: 2024-09-20T00:00:00-04:00<a href="http://arxiv.org/pdf/2409.11383v1"><h2>Training Datasets Generation for Machine Learning: Application to Vision
  Based Navigation</h2></a>Authors:  Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco</br>Comments: 6 pages, 4 figures, preprint of the proceedings of ESA SPAICE
  conference 2024</br>Primary Category: cs.CV</br>All Categories: cs.CV, astro-ph.EP, cs.GR, cs.LG</br><p>Vision Based Navigation consists in utilizing cameras as precision sensors
for GNC after extracting information from images. To enable the adoption of
machine learning for space applications, one of obstacles is the demonstration
that available training datasets are adequate to validate the algorithms. The
objective of the study is to generate datasets of images and metadata suitable
for training machine learning algorithms. Two use cases were selected and a
robust methodology was developed to validate the datasets including the ground
truth. The first use case is in-orbit rendezvous with a man-made object: a
mockup of satellite ENVISAT. The second use case is a Lunar landing scenario.
Datasets were produced from archival datasets (Chang'e 3), from the laboratory
at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software
high fidelity image simulator using Model Capture and from Generative
Adversarial Networks. The use case definition included the selection of
algorithms as benchmark: an AI-based pose estimation algorithm and a dense
optical flow algorithm were selected. Eventually it is demonstrated that
datasets produced with SurRender and selected laboratory facilities are
adequate to train machine learning algorithms.</p></br><a href="http://arxiv.org/pdf/2409.09563v1"><h2>Astrometric Binary Classification Via Artificial Neural Networks</h2></a>Authors:  Joe Smith</br>Comments: Accepted for publication in Astrophysical Journal (ApJ)</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.LG, physics.data-an</br><p>With nearly two billion stars observed and their corresponding astrometric
parameters evaluated in the recent Gaia mission, the number of astrometric
binary candidates have risen significantly. Due to the surplus of astrometric
data, the current computational methods employed to inspect these astrometric
binary candidates are both computationally expensive and cannot be executed in
a reasonable time frame. In light of this, a machine learning (ML) technique to
automatically classify whether a set of stars belong to an astrometric binary
pair via an artificial neural network (ANN) is proposed. Using data from Gaia
DR3, the ANN was trained and tested on 1.5 million highly probable true and
visual binaries, considering the proper motions, parallaxes, and angular and
physical separations as features. The ANN achieves high classification scores,
with an accuracy of 99.3%, a precision rate of 0.988, a recall rate of 0.991,
and an AUC of 0.999, indicating that the utilized ML technique is a highly
effective method for classifying astrometric binaries. Thus, the proposed ANN
is a promising alternative to the existing methods for the classification of
astrometric binaries.</p></br><a href="http://arxiv.org/pdf/2409.10381v1"><h2>J-UBIK: The JAX-accelerated Universal Bayesian Imaging Kit</h2></a>Authors:  Vincent Eberle, Matteo Guardiani, Margret Westerkamp, Philipp Frank, Julian Rüstig, Julia Stadler, Torsten A. Enßlin</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, physics.comp-ph, physics.data-an, physics.ins-det</br><p>Many advances in astronomy and astrophysics originate from accurate images of
the sky emission across multiple wavelengths. This often requires
reconstructing spatially and spectrally correlated signals detected from
multiple instruments. To facilitate the high-fidelity imaging of these signals,
we introduce the universal Bayesian imaging kit (UBIK). Specifically, we
present J-UBIK, a flexible and modular implementation leveraging the
JAX-accelerated NIFTy.re software as its backend. J-UBIK streamlines the
implementation of the key Bayesian inference components, providing for all the
necessary steps of Bayesian imaging pipelines. First, it provides adaptable
prior models for different sky realizations. Second, it includes likelihood
models tailored to specific instruments. So far, the package includes three
instruments: Chandra and eROSITA for X-ray observations, and the James Webb
Space Telescope (JWST) for the near- and mid-infrared. The aim is to expand
this set in the future. Third, these models can be integrated with various
inference and optimization schemes, such as maximum a posteriori estimation and
variational inference. Explicit demos show how to integrate the individual
modules into a full analysis pipeline. Overall, J-UBIK enables efficient
generation of high-fidelity images via Bayesian pipelines that can be tailored
to specific research objectives.</p></br>
