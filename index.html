search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202502282000+TO+202503062000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, cs.LG, cs.AI, physics.data-an staritng 202502282000 and ending 202503062000</h1>Feed last updated: 2025-03-05T00:00:00-05:00<a href="http://arxiv.org/pdf/2503.02456v1"><h2>Inferring Galactic Parameters from Chemical Abundances with
  Simulation-Based Inference</h2></a>Authors:  Tobias Buck, Berkay Günes, Giuseppe Viterbo, William H. Oliver, Sven Buder</br>Comments: submitted to A&A, comments welcome, all source code to reproduce this
  work can be found on GitHub under url: https://github.com/TobiBu/sbi-chempy</br>Primary Category: astro-ph.GA</br>All Categories: astro-ph.GA, astro-ph.IM, physics.comp-ph, physics.data-an, physics.space-ph</br><p>Galactic chemical abundances provide crucial insights into fundamental
galactic parameters, such as the high-mass slope of the initial mass function
(IMF) and the normalization of Type Ia supernova (SN Ia) rates. Constraining
these parameters is essential for advancing our understanding of stellar
feedback, metal enrichment, and galaxy formation processes. However,
traditional Bayesian inference techniques, such as Hamiltonian Monte Carlo
(HMC), are computationally prohibitive when applied to large datasets of modern
stellar surveys. We leverage simulation-based-inference (SBI) as a scalable,
robust, and efficient method for constraining galactic parameters from stellar
chemical abundances and demonstrate its the advantages over HMC in terms of
speed, scalability, and robustness against model misspecifications. We combine
a Galactic Chemical Evolution (GCE) model, CHEMPY, with a neural network
emulator and a Neural Posterior Estimator (NPE) to train our SBI pipeline. Mock
datasets are generated using CHEMPY, including scenarios with mismatched
nucleosynthetic yields, with additional tests conducted on data from a
simulated Milky Way-like galaxy. SBI results are benchmarked against HMC-based
inference, focusing on computational performance, accuracy, and resilience to
systematic discrepancies. SBI achieves a $\sim75,600\times$ speed-up compared
to HMC, reducing inference runtime from $\gtrsim42$ hours to mere seconds for
thousands of stars. Inference on $1,000$ stars yields precise estimates for the
IMF slope ($\alpha_{\rm IMF} = -2.298 \pm 0.002$) and SN Ia normalization
($\log_{10}(N_{\rm Ia}) = -2.885 \pm 0.003$), deviating less than 0.05% from
the ground truth. SBI also demonstrates similar robustness to model
misspecification than HMC, recovering accurate parameters even with alternate
yield tables or data from a cosmological simulation. (shortened...)</p></br><a href="http://arxiv.org/pdf/2503.00821v1"><h2>AI Agents for Ground-Based Gamma Astronomy</h2></a>Authors:  D. Kostunin, V. Sotnikov, S. Golovachev, A. Strube</br>Comments: proceedings of ADASS2025 (submitted), original talk and demos:
  https://pretalx.com/adass2024/talk/TRFZKU/</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI</br><p>Next-generation instruments for ground-based gamma-ray astronomy are marked
by a substantial increase in complexity, featuring dozens of telescopes. This
leap in scale introduces significant challenges in managing system operations
and offline data analysis. Methods, which depend on advanced personnel training
and sophisticated software, become increasingly strained as system complexity
grows, making it more challenging to effectively support users in such a
multifaceted environment. To address these challenges, we propose the
development of AI agents based on instruction-finetuned large language models
(LLMs). These agents align with specific documentation and codebases,
understand the environmental context, operate with external APIs, and
communicate with humans in natural language. Leveraging the advanced
capabilities of modern LLMs, which can process and retain vast amounts of
information, these AI agents offer a transformative approach to system
management and data analysis by automating complex tasks and providing
intelligent assistance. We present two prototypes that integrate with the
Cherenkov Telescope Array Observatory pipelines for operations and offline data
analysis. The first prototype automates data model implementation and
maintenance for the Configuration Database of the Array Control and Data
Acquisition (ACADA). The second prototype is an open-access code generation
application tailored for data analysis based on the Gammapy framework.</p></br><a href="http://arxiv.org/pdf/2503.02112v1"><h2>Building Machine Learning Challenges for Anomaly Detection in Science</h2></a>Authors:  Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Saúl Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, Yibo Zhong</br>Comments: 18 pages 6 figures to be submitted to Nature Communications</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM</br><p>Scientific discoveries are often made by finding a pattern or object that was
not predicted by the known rules of science. Oftentimes, these anomalous events
or objects that do not conform to the norms are an indication that the rules of
science governing the data are incomplete, and something new needs to be
present to explain these unexpected outliers. The challenge of finding
anomalies can be confounding since it requires codifying a complete knowledge
of the known scientific behaviors and then projecting these known behaviors on
the data to look for deviations. When utilizing machine learning, this presents
a particular challenge since we require that the model not only understands
scientific data perfectly but also recognizes when the data is inconsistent and
out of the scope of its trained behavior. In this paper, we present three
datasets aimed at developing machine learning-based anomaly detection for
disparate scientific domains covering astrophysics, genomics, and polar
science. We present the different datasets along with a scheme to make machine
learning challenges around the three datasets findable, accessible,
interoperable, and reusable (FAIR). Furthermore, we present an approach that
generalizes to future machine learning challenges, enabling the possibility of
large, more compute-intensive challenges that can ultimately lead to scientific
discovery.</p></br><a href="http://arxiv.org/pdf/2503.02554v1"><h2>Towards a robust R2D2 paradigm for radio-interferometric imaging:
  revisiting DNN training and architecture</h2></a>Authors:  Amir Aghabiglou, Chung San Chu, Chao Tang, Arwa Dabbech, Yves Wiaux</br>Comments: 17 pages, 6 figures</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.CV, cs.LG</br><p>The R2D2 Deep Neural Network (DNN) series was recently introduced for image
formation in radio interferometry. It can be understood as a learned version of
CLEAN, whose minor cycles are substituted with DNNs. We revisit R2D2 on the
grounds of series convergence, training methodology, and DNN architecture,
improving its robustness in terms of generalisability beyond training
conditions, capability to deliver high data fidelity, and epistemic
uncertainty. Firstly, while still focusing on telescope-specific training, we
enhance the learning process by randomising Fourier sampling integration times,
incorporating multi-scan multi-noise configurations, and varying imaging
settings, including pixel resolution and visibility-weighting scheme. Secondly,
we introduce a convergence criterion whereby the reconstruction process stops
when the data residual is compatible with noise, rather than simply using all
available DNNs. This not only increases the reconstruction efficiency by
reducing its computational cost, but also refines training by pruning out the
data/image pairs for which optimal data fidelity is reached before training the
next DNN. Thirdly, we substitute R2D2's early U-Net DNN with a novel
architecture (U-WDSR) combining U-Net and WDSR, which leverages wide
activation, dense connections, weight normalisation, and low-rank convolution
to improve feature reuse and reconstruction precision. As previously, R2D2 was
trained for monochromatic intensity imaging with the Very Large Array (VLA) at
fixed $512 \times 512$ image size. Simulations on a wide range of inverse
problems and a case study on real data reveal that the new R2D2 model
consistently outperforms its earlier version in image reconstruction quality,
data fidelity, and epistemic uncertainty.</p></br><a href="http://arxiv.org/pdf/2503.00156v1"><h2>Neural Posterior Estimation for Cataloging Astronomical Images with
  Spatially Varying Backgrounds and Point Spread Functions</h2></a>Authors:  Aakash Patel, Tianqing Zhang, Camille Avestruz, Jeffrey Regier, the LSST Dark Energy Science Collaboration</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.CV, cs.LG, eess.IV, stat.AP, 85A35, 62F15, J.2; I.2.10</br><p>Neural posterior estimation (NPE), a type of amortized variational inference,
is a computationally efficient means of constructing probabilistic catalogs of
light sources from astronomical images. To date, NPE has not been used to
perform inference in models with spatially varying covariates. However,
ground-based astronomical images have spatially varying sky backgrounds and
point spread functions (PSFs), and accounting for this variation is essential
for constructing accurate catalogs of imaged light sources. In this work, we
introduce a method of performing NPE with spatially varying backgrounds and
PSFs. In this method, we generate synthetic catalogs and semi-synthetic images
for these catalogs using randomly sampled PSF and background estimates from
existing surveys. Using this data, we train a neural network, which takes an
astronomical image and representations of its background and PSF as input, to
output a probabilistic catalog. Our experiments with Sloan Digital Sky Survey
data demonstrate the effectiveness of NPE in the presence of spatially varying
backgrounds and PSFs for light source detection, star/galaxy separation, and
flux measurement.</p></br><a href="http://arxiv.org/pdf/2503.01462v1"><h2>S-R2D2: a spherical extension of the R2D2 deep neural network series
  paradigm for wide-field radio-interferometric imaging</h2></a>Authors:  A. Tajja, A. Aghabiglou, E. Tolley, J-P. Kneib, J-P. Thiran, Y. Wiaux</br>Comments: 16 pages, 13 figures</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.CV, cs.LG, eess.IV, eess.SP</br><p>Recently, the R2D2 paradigm, standing for ''Residual-to-Residual DNN series
for high-Dynamic-range imaging'', was introduced for image formation in Radio
Interferometry (RI) as a learned version of the traditional algorithm CLEAN.
The first incarnations of R2D2 are limited to planar imaging on small fields of
view, failing to meet the spherical-imaging requirement of modern telescopes
observing wide fields. To address this limitation, we propose the
spherical-imaging extension S-R2D2. Firstly, as R2D2, S-R2D2 encapsulates its
minor cycles in existing 2D-Euclidean deep neural network (DNN) architectures,
but adapts its iterative scheme to incorporate the wide-field measurement model
mapping a spherical image to visibility data. We implemented this model as the
composition of an efficient Fourier-based interpolator mapping the spherical
image onto the equatorial plane, with the standard RI operator mapping the
equatorial-plane image to visibility data. Importantly, the interpolation step
must inevitably be performed at a lower-than-optimal resolution on the plane,
to meet the high-resolution requirement on the sphere of wide-field imaging
while preserving scalability. Therefore, secondly, we design S-R2D2's DNN
training loss to jointly learn to correct the interpolation approximations and
identify residual image structures on the sphere, ensuring consistency with the
spherical ground truth using the adjoint plane-to-sphere interpolator. Finally,
we demonstrate through simulations S-R2D2's capability to perform fast and
accurate reconstructions of spherical monochromatic intensity images, across
high-resolution, high-dynamic-range settings.</p></br><a href="http://arxiv.org/pdf/2503.02880v1"><h2>A New $\sim 5σ$ Tension at Characteristic Redshift from DESI DR1
  and DES-SN5YR observations</h2></a>Authors:  Purba Mukherjee, Anjan A Sen</br>Comments: 4 pages, 1 table, 1 figure. Comments are welcome</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, cs.LG, gr-qc, hep-th</br><p>We perform a model-independent reconstruction of the angular diameter
distance ($D_{A}$) using the Multi-Task Gaussian Process (MTGP) framework with
DESI-DR1 BAO and DES-SN5YR datasets. We calibrate the comoving sound horizon at
the baryon drag epoch $r_d$ to the Planck best-fit value, ensuring consistency
with early-universe physics. With the reconstructed $D_A$ at two key redshifts,
$z\sim 1.63$ (where $D_{A}^{\prime} =0$) and at $z\sim 0.512$ (where
$D_{A}^{\prime} = D_{A}$), we derive the expansion rate of the Universe $H(z)$
at these redshifts. Our findings reveal that at $z\sim 1.63$, the $H(z)$ is
fully consistent with the Planck-2018 $\Lambda$CDM prediction, confirming no
new physics at that redshift. However, at $z \sim 0.512$, the derived $H(z)$
shows a more than $5\sigma$ discrepancy with the Planck-2018 $\Lambda$CDM
prediction, suggesting a possible breakdown of the $\Lambda$CDM model as
constrained by Planck-2018 at this lower redshift. This emerging $\sim 5\sigma$
tension at $z\sim 0.512$, distinct from the existing ``Hubble Tension'', may
signal the first strong evidence for new physics at low redshifts.</p></br>
