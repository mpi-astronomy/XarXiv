search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202410092000+TO+202410152000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.AI, physics.data-an, stat.*, cs.LG staritng 202410092000 and ending 202410152000</h1>Feed last updated: 2024-10-15T00:00:00-04:00<a href="http://arxiv.org/pdf/2410.08923v1"><h2>Path-minimizing Latent ODEs for improved extrapolation and inference</h2></a>Authors:  Matt L. Sampson, Peter Melchior</br>Comments: 20 pages 11 figures</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM</br><p>Latent ODE models provide flexible descriptions of dynamic systems, but they
can struggle with extrapolation and predicting complicated non-linear dynamics.
The latent ODE approach implicitly relies on encoders to identify unknown
system parameters and initial conditions, whereas the evaluation times are
known and directly provided to the ODE solver. This dichotomy can be exploited
by encouraging time-independent latent representations. By replacing the common
variational penalty in latent space with an $\ell_2$ penalty on the path length
of each system, the models learn data representations that can easily be
distinguished from those of systems with different configurations. This results
in faster training, smaller models, more accurate interpolation and long-time
extrapolation compared to the baseline ODE models with GRU, RNN, and LSTM
encoder/decoders on tests with damped harmonic oscillator, self-gravitating
fluid, and predator-prey systems. We also demonstrate superior results for
simulation-based inference of the Lotka-Volterra parameters and initial
conditions by using the latents as data summaries for a conditional normalizing
flow. Our change to the training loss is agnostic to the specific recognition
network used by the decoder and can therefore easily be adopted by other latent
ODE models.</p></br><a href="http://arxiv.org/pdf/2410.07548v1"><h2>Hybrid Summary Statistics</h2></a>Authors:  T. Lucas Makinen, Ce Sui, Benjamin D. Wandelt, Natalia Porqueres, Alan Heavens</br>Comments: 7 pages, 4 figures. Accepted to ML4PS2024 at NeurIPS 2024</br>Primary Category: stat.ML</br>All Categories: stat.ML, astro-ph.CO, cs.IT, cs.LG, math.IT, physics.data-an</br><p>We present a way to capture high-information posteriors from training sets
that are sparsely sampled over the parameter space for robust simulation-based
inference. In physical inference problems, we can often apply domain knowledge
to define traditional summary statistics to capture some of the information in
a dataset. We show that augmenting these statistics with neural network outputs
to maximise the mutual information improves information extraction compared to
neural summaries alone or their concatenation to existing summaries and makes
inference robust in settings with low training data. We introduce 1) two loss
formalisms to achieve this and 2) apply the technique to two different
cosmological datasets to extract non-Gaussian parameter information.</p></br>
