search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202602112000+TO+202602172000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.AI, physics.data-an, stat.*, cs.LG staritng 202602112000 and ending 202602172000</h1>Feed last updated: 2026-02-17T05:16:50Z<a href="https://arxiv.org/pdf/2602.15021v1"><h2>Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI</h2></a>Authors:  Xiaosheng Zhao, Yuan-Sen Ting, Rosemary F. G. Wyse, Alexander S. Szalay, Yang Huang, L치szl칩 Dobos, Tam치s Budav치ri, Viska Wei</br>Comments: 20 pages, 13 figures, 4 tables. Submitted to AAS journals. Comments welcome</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, astro-ph.GA, cs.LG</br><p>Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] > -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.</p></br>
