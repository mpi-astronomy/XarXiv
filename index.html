search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202504102000+TO+202504162000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, physics.data-an, cs.AI, stat.* staritng 202504102000 and ending 202504162000</h1>Feed last updated: 2025-04-16T00:00:00-04:00<a href="http://arxiv.org/pdf/2504.10550v1"><h2>LCDC: Bridging Science and Machine Learning for Light Curve Analysis</h2></a>Authors:  Daniel Kyselica, Tomáš Hrobár, Jiří Šilha, Roman Ďurikovič, Marek Šuppa</br>Comments: 13 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:2412.00544</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.EP, cs.LG</br><p>The characterization and analysis of light curves are vital for understanding
the physical and rotational properties of artificial space objects such as
satellites, rocket stages, and space debris. This paper introduces the Light
Curve Dataset Creator (LCDC), a Python-based toolkit designed to facilitate the
preprocessing, analysis, and machine learning applications of light curve data.
LCDC enables seamless integration with publicly available datasets, such as the
newly introduced Mini Mega Tortora (MMT) database. Moreover, it offers data
filtering, transformation, as well as feature extraction tooling. To
demonstrate the toolkit's capabilities, we created the first standardized
dataset for rocket body classification, RoBo6, which was used to train and
evaluate several benchmark machine learning models, addressing the lack of
reproducibility and comparability in recent studies. Furthermore, the toolkit
enables advanced scientific analyses, such as surface characterization of the
Atlas 2AS Centaur and the rotational dynamics of the Delta 4 rocket body, by
streamlining data preprocessing, feature extraction, and visualization. These
use cases highlight LCDC's potential to advance space debris characterization
and promote sustainable space exploration. Additionally, they highlight the
toolkit's ability to enable AI-focused research within the space debris
community.</p></br><a href="http://arxiv.org/pdf/2504.08928v1"><h2>Observability of Acausal and Uncorrelated Optical-Quasar Pairs for
  Quantum-Mechanical Experiments</h2></a>Authors:  Eric Steinbring</br>Comments: 16 pages, 13 figures; accepted for publication in Universe</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, physics.data-an, quant-ph</br><p>Viewing high-redshift sources at near-opposite directions on the sky can
assure, by light-travel-time arguments, acausality between their emitted
photons. One utility would be true random-number generation, by sensing these
via two independent telescopes that each flip a switch based on those
latest-arrived colours; for example, to autonomously control a
quantum-mechanical (QM) experiment. Although demonstrated with distant quasars,
those were not fully acausal pairs, which are restricted in simultaneous view
from the ground at any single observatory. In optical light such faint sources
also require large telescope aperture to avoid sampling assumptions when imaged
at fast camera framerates: either unsensed intrinsic correlations between them
or equivalently-correlated noise may ruin the expectation of pure randomness.
One such case which could spoil a QM test is considered. Based on that, allowed
geometries and instrumental limits are modelled for any two ground-based sites,
and their data simulated. To compare, an analysis of photometry from the Gemini
twin 8-m telescopes is presented, using archival data of well-separated bright
stars, obtained with the instruments 'Alopeke (on Gemini-North in Hawai'i) and
Zorro (on Gemini-South in Chile) simultaneously in two bands (centred at 562 nm
and 832 nm) with 17 Hz framerate. No flux correlation is found, calibrating an
analytic model, predicting where a search at signal-to-noise over 50 at 50 Hz
with the same instrumentation can be made. Finally, the software PDQ (Predict
Different Quasars) is presented which searches a large catalogue of known
quasars, reporting those with brightness and visibility suitable to verify
acausal, uncorrelated photons at those limits.</p></br><a href="http://arxiv.org/pdf/2504.10453v1"><h2>Anchors no more: Using peculiar velocities to constrain $H_0$ and the
  primordial Universe without calibrators</h2></a>Authors:  Davide Piras, Francesco Sorrenti, Ruth Durrer, Martin Kunz</br>Comments: 22 pages, 5 figures, comments welcome. Code available at
  https://github.com/dpiras/veloce</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, astro-ph.IM, cs.LG, gr-qc</br><p>We develop a novel approach to constrain the Hubble parameter $H_0$ and the
primordial power spectrum amplitude $A_\mathrm{s}$ using supernovae type Ia
(SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we
can model their distance and their covariance as a function of cosmological
parameters without the need of calibrators like Cepheids; this yields a new
independent probe of the large-scale structure based on SNIa data without
distance anchors. Crucially, we implement a differentiable pipeline in JAX,
including efficient emulators and affine sampling, reducing inference time from
years to hours on a single GPU. We first validate our method on mock datasets,
demonstrating that we can constrain $H_0$ and $\log 10^{10}A_\mathrm{s}$ within
$\sim10\%$ using $\sim10^3$ SNIa. We then test our pipeline with SNIa from an
$N$-body simulation, obtaining $7\%$-level unbiased constraints on $H_0$ with a
moderate noise level. We finally apply our method to Pantheon+ data,
constraining $H_0$ at the $10\%$ level without Cepheids when fixing
$A_\mathrm{s}$ to its $\it{Planck}$ value. On the other hand, we obtain
$15\%$-level constraints on $\log 10^{10}A_\mathrm{s}$ in agreement with
$\it{Planck}$ when including Cepheids in the analysis. In light of upcoming
observations of low redshift SNIa from the Zwicky Transient Facility and the
Vera Rubin Legacy Survey of Space and Time, surveys for which our method will
develop its full potential, we make our code publicly available.</p></br><a href="http://arxiv.org/pdf/2504.10553v1"><h2>Inferring the Hubble Constant Using Simulated Strongly Lensed Supernovae
  and Neural Network Ensembles</h2></a>Authors:  Gonçalo Gonçalves, Nikki Arendse, Doogesh Kodi Ramanah, Radosław Wojtak</br>Comments: 12 pages, 9 figures. To be submitted to the Open Journal of
  Astrophysics</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, cs.LG</br><p>Strongly lensed supernovae are a promising new probe to obtain independent
measurements of the Hubble constant (${H_0}$). In this work, we employ
simulated gravitationally lensed Type Ia supernovae (glSNe Ia) to train our
machine learning (ML) pipeline to constrain $H_0$. We simulate image
time-series of glSNIa, as observed with the upcoming Nancy Grace Roman Space
Telescope, that we employ for training an ensemble of five convolutional neural
networks (CNNs). The outputs of this ensemble network are combined with a
simulation-based inference (SBI) framework to quantify the uncertainties on the
network predictions and infer full posteriors for the $H_0$ estimates. We
illustrate that the combination of multiple glSN systems enhances constraint
precision, providing a $4.4\%$ estimate of $H_0$ based on 100 simulated
systems, which is in agreement with the ground truth. This research highlights
the potential of leveraging the capabilities of ML with glSNe systems to obtain
a pipeline capable of fast and automated $H_0$ measurements.</p></br><a href="http://arxiv.org/pdf/2504.08583v1"><h2>AstroLLaVA: towards the unification of astronomical data and natural
  language</h2></a>Authors:  Sharaf Zaman, Michael J. Smith, Pranav Khetarpal, Rishabh Chakrabarty, Michele Ginolfi, Marc Huertas-Company, Maja Jabłońska, Sandor Kruk, Matthieu Le Lain, Sergio José Rodríguez Méndez, Dimitrios Tanoglidis</br>Comments: 8 pages, 3 figures, accepted to SCI-FM@ICLR 2025. Code at
  https://w3id.org/UniverseTBD/AstroLLaVA</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.LG</br><p>We present AstroLLaVA, a vision language model for astronomy that enables
interaction with astronomical imagery through natural dialogue. By fine-tuning
the LLaVA model on a diverse dataset of $\sim$30k images with captions and
question-answer pairs sourced from NASA's `Astronomy Picture of the Day', the
European Southern Observatory, and the NASA/ESA Hubble Space Telescope, we
create a model capable of answering open-ended questions about astronomical
concepts depicted visually. Our two-stage fine-tuning process adapts the model
to both image captioning and visual question answering in the astronomy domain.
We demonstrate AstroLLaVA's performance on an astronomical visual question
answering benchmark and release the model weights, code, and training set to
encourage further open source work in this space. Finally, we suggest a roadmap
towards general astronomical data alignment with pre-trained language models,
and provide an open space for collaboration towards this end for interested
researchers.</p></br><a href="http://arxiv.org/pdf/2504.09449v1"><h2>aweSOM: a CPU/GPU-accelerated Self-organizing Map and Statistically
  Combined Ensemble Framework for Machine-learning Clustering Analysis</h2></a>Authors:  Trung Ha, Joonas Nättilä, Jordy Davelaar</br>Comments: Published in the Journal of Open Source Software; method paper for
  arXiv: 2410.01878</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, stat.ML</br><p>We introduce aweSOM, an open-source Python package for machine learning (ML)
clustering and classification, using a Self-organizing Maps (SOM) algorithm
that incorporates CPU/GPU acceleration to accommodate large ($N > 10^6$, where
$N$ is the number of data points), multidimensional datasets. aweSOM consists
of two main modules, one that handles the initialization and training of the
SOM, and another that stacks the results of multiple SOM realizations to obtain
more statistically robust clusters. Existing Python-based SOM implementations
(e.g., POPSOM, Yuan (2018); MiniSom, Vettigli (2018); sklearn-som) primarily
serve as proof-of-concept demonstrations, optimized for smaller datasets, but
lacking scalability for large, multidimensional data. aweSOM provides a
solution for this gap in capability, with good performance scaling up to $\sim
10^8$ individual points, and capable of utilizing multiple features per point.
We compare the code performance against the legacy implementations it is based
on, and find a 10-100x speed up, as well as significantly improved memory
efficiency, due to several built-in optimizations.</p></br>
