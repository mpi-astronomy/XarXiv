search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202410302000+TO+202411052000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, stat.*, cs.AI, physics.data-an staritng 202410302000 and ending 202411052000</h1>Feed last updated: 2024-11-04T00:00:00-05:00<a href="http://arxiv.org/pdf/2411.00991v1"><h2>Re-thinking Richardson-Lucy without Iteration Cutoffs: Physically
  Motivated Bayesian Deconvolution</h2></a>Authors:  Zachary H. Hendrix, Peter T. Brown, Tim Flanagan, Douglas P. Shepherd, Ayush Saurabh, Steve Press√©</br>Comments: 5 figures</br>Primary Category: cs.CV</br>All Categories: cs.CV, astro-ph.IM, physics.bio-ph, physics.data-an, physics.optics</br><p>Richardson-Lucy deconvolution is widely used to restore images from
degradation caused by the broadening effects of a point spread function and
corruption by photon shot noise, in order to recover an underlying object. In
practice, this is achieved by iteratively maximizing a Poisson emission
likelihood. However, the RL algorithm is known to prefer sparse solutions and
overfit noise, leading to high-frequency artifacts. The structure of these
artifacts is sensitive to the number of RL iterations, and this parameter is
typically hand-tuned to achieve reasonable perceptual quality of the inferred
object. Overfitting can be mitigated by introducing tunable regularizers or
other ad hoc iteration cutoffs in the optimization as otherwise incorporating
fully realistic models can introduce computational bottlenecks. To resolve
these problems, we present Bayesian deconvolution, a rigorous deconvolution
framework that combines a physically accurate image formation model avoiding
the challenges inherent to the RL approach. Our approach achieves deconvolution
while satisfying the following desiderata:
  I deconvolution is performed in the spatial domain (as opposed to the
frequency domain) where all known noise sources are accurately modeled and
integrated in the spirit of providing full probability distributions over the
density of the putative object recovered;
  II the probability distribution is estimated without making assumptions on
the sparsity or continuity of the underlying object;
  III unsupervised inference is performed and converges to a stable solution
with no user-dependent parameter tuning or iteration cutoff;
  IV deconvolution produces strictly positive solutions; and
  V implementation is amenable to fast, parallelizable computation.</p></br>
