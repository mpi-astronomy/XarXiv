search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202510012000+TO+202510072000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, cs.AI, cs.LG, physics.data-an staritng 202510012000 and ending 202510072000</h1>Feed last updated: 2025-10-07T00:00:00-04:00<a href="http://arxiv.org/pdf/2510.01799v1"><h2>PRESOL: a web-based computational setting for feature-based flare
  forecasting</h2></a>Authors:  Chiara Curletto, Paolo Massa, Valeria Tagliafico, Cristina Campi, Federico Benvenuto, Michele Piana, Andrea Tacchino</br>Comments: No comment found</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, astro-ph.IM, cs.LG, physics.space-ph, 68T07, 85-04</br><p>Solar flares are the most explosive phenomena in the solar system and the
main trigger of the events' chain that starts from Coronal Mass Ejections and
leads to geomagnetic storms with possible impacts on the infrastructures at
Earth. Data-driven solar flare forecasting relies on either deep learning
approaches, which are operationally promising but with a low explainability
degree, or machine learning algorithms, which can provide information on the
physical descriptors that mostly impact the prediction. This paper describes a
web-based technological platform for the execution of a computational pipeline
of feature-based machine learning methods that provide predictions of the flare
occurrence, feature ranking information, and assessment of the prediction
performances.</p></br><a href="http://arxiv.org/pdf/2510.02527v1"><h2>Self-supervised diffusion model fine-tuning for costate initialization
  using Markov chain Monte Carlo</h2></a>Authors:  Jannik Graebner, Ryne Beeson</br>Comments: No comment found</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, astro-ph.IM, cs.LG, cs.SY, eess.SY, math.OC</br><p>Global search and optimization of long-duration, low-thrust spacecraft
trajectories with the indirect method is challenging due to a complex solution
space and the difficulty of generating good initial guesses for the costate
variables. This is particularly true in multibody environments. Given data that
reveals a partial Pareto optimal front, it is desirable to find a flexible
manner in which the Pareto front can be completed and fronts for related
trajectory problems can be found. In this work we use conditional diffusion
models to represent the distribution of candidate optimal trajectory solutions.
We then introduce into this framework the novel approach of using Markov Chain
Monte Carlo algorithms with self-supervised fine-tuning to achieve the
aforementioned goals. Specifically, a random walk Metropolis algorithm is
employed to propose new data that can be used to fine-tune the diffusion model
using a reward-weighted training based on efficient evaluations of constraint
violations and missions objective functions. The framework removes the need for
separate focused and often tedious data generation phases. Numerical
experiments are presented for two problems demonstrating the ability to improve
sample quality and explicitly target Pareto optimality based on the theory of
Markov chains. The first problem does so for a transfer in the Jupiter-Europa
circular restricted three-body problem, where the MCMC approach completes a
partial Pareto front. The second problem demonstrates how a dense and superior
Pareto front can be generated by the MCMC self-supervised fine-tuning method
for a Saturn-Titan transfer starting from the Jupiter-Europa case versus a
separate dedicated global search.</p></br><a href="http://arxiv.org/pdf/2510.01733v1"><h2>Reducing Simulation Dependence in Neutrino Telescopes with Masked Point
  Transformers</h2></a>Authors:  Felix J. Yu, Nicholas Kamp, Carlos A. Argüelles</br>Comments: 8 pages, 3 figures, presented at the 39th International Cosmic Ray
  Conference (ICRC2025)</br>Primary Category: hep-ex</br>All Categories: hep-ex, astro-ph.IM, cs.LG</br><p>Machine learning techniques in neutrino physics have traditionally relied on
simulated data, which provides access to ground-truth labels. However, the
accuracy of these simulations and the discrepancies between simulated and real
data remain significant concerns, particularly for large-scale neutrino
telescopes that operate in complex natural media. In recent years,
self-supervised learning has emerged as a powerful paradigm for reducing
dependence on labeled datasets. Here, we present the first self-supervised
training pipeline for neutrino telescopes, leveraging point cloud transformers
and masked autoencoders. By shifting the majority of training to real data,
this approach minimizes reliance on simulations, thereby mitigating associated
systematic uncertainties. This represents a fundamental departure from previous
machine learning applications in neutrino telescopes, paving the way for
substantial improvements in event reconstruction and classification.</p></br><a href="http://arxiv.org/pdf/2510.04762v1"><h2>Fisher-Bingham-like normalizing flows on the sphere</h2></a>Authors:  Thorsten Glüsenkamp</br>Comments: No comment found</br>Primary Category: stat.ML</br>All Categories: stat.ML, astro-ph.IM, cs.AI, cs.LG</br><p>A generic D-dimensional Gaussian can be conditioned or projected onto the D-1
unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular
Gaussian (AG) distribution families, respectively. These are some of the most
fundamental distributions on the sphere, yet cannot straightforwardly be
written as a normalizing flow except in two special cases: the von-Mises Fisher
in D=3 and the central angular Gaussian in any D. In this paper, we describe
how to generalize these special cases to a family of normalizing flows that
behave similarly to the full FB or AG family in any D. We call them
"zoom-linear-project" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham
distribution, their composition allows to gradually add complexity as needed.
Furthermore, they can naturally handle conditional density estimation with
target distributions that vary by orders of magnitude in scale - a setting that
is important in astronomical applications but that existing flows often
struggle with. A particularly useful member of the new family is the Kent
analogue that can cheaply upgrade any flow in this situation to yield better
performance.</p></br><a href="http://arxiv.org/pdf/2510.05016v1"><h2>Large Language Models Achieve Gold Medal Performance at International
  Astronomy & Astrophysics Olympiad</h2></a>Authors:  Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun</br>Comments: 18 pages, 6 figures, to be submitted, comments are welcome</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI, cs.CL</br><p>While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.</p></br>
