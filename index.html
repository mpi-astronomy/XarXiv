search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202408282000+TO+202409032000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, cs.AI, stat.*, physics.data-an staritng 202408282000 and ending 202409032000</h1>Feed last updated: 2024-09-03T00:00:00-04:00<a href="http://arxiv.org/pdf/2408.16829v1"><h2>Maven: A Multimodal Foundation Model for Supernova Science</h2></a>Authors:  Gemma Zhang, Thomas Helfer, Alexander T. Gagliano, Siddharth Mishra-Sharma, V. Ashley Villar</br>Comments: code: https://github.com/ThomasHelfer/multimodal-supernovae data:
  https://huggingface.co/datasets/thelfer/multimodal_supernovae</br>Primary Category: astro-ph.HE</br>All Categories: astro-ph.HE, astro-ph.IM, cs.LG</br><p>A common setting in astronomy is the availability of a small number of
high-quality observations, and larger amounts of either lower-quality
observations or synthetic data from simplified models. Time-domain astrophysics
is a canonical example of this imbalance, with the number of supernovae
observed photometrically outpacing the number observed spectroscopically by
multiple orders of magnitude. At the same time, no data-driven models exist to
understand these photometric and spectroscopic observables in a common context.
Contrastive learning objectives, which have grown in popularity for aligning
distinct data modalities in a shared embedding space, provide a potential
solution to extract information from these modalities. We present Maven, the
first foundation model for supernova science. To construct Maven, we first
pre-train our model to align photometry and spectroscopy from 0.5M synthetic
supernovae using a constrastive objective. We then fine-tune the model on 4,702
observed supernovae from the Zwicky Transient Facility. Maven reaches
state-of-the-art performance on both classification and redshift estimation,
despite the embeddings not being explicitly optimized for these tasks. Through
ablation studies, we show that pre-training with synthetic data improves
overall performance. In the upcoming era of the Vera C. Rubin Observatory,
Maven serves as a Rosetta Stone for leveraging large, unlabeled and multimodal
time-domain datasets.</p></br><a href="http://arxiv.org/pdf/2408.16212v1"><h2>The Application of Machine Learning in Tidal Evolution Simulation of
  Star-Planet Systems</h2></a>Authors:  Shuaishuai Guo, Jianheng Guo, KaiFan Ji, Hui Liu, Lei Xing</br>Comments: No comment found</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, astro-ph.SR, cs.LG</br><p>With the release of a large amount of astronomical data, an increasing number
of close-in hot Jupiters have been discovered. Calculating their evolutionary
curves using star-planet interaction models presents a challenge. To expedite
the generation of evolutionary curves for these close-in hot Jupiter systems,
we utilized tidal interaction models established on MESA to create 15,745
samples of star-planet systems and 7,500 samples of stars. Additionally, we
employed a neural network (Multi-Layer Perceptron - MLP) to predict the
evolutionary curves of the systems, including stellar effective temperature,
radius, stellar rotation period, and planetary orbital period. The median
relative errors of the predicted evolutionary curves were found to be 0.15%,
0.43%, 2.61%, and 0.57%, respectively. Furthermore, the speed at which we
generate evolutionary curves exceeds that of model-generated curves by more
than four orders of magnitude. We also extracted features of planetary
migration states and utilized lightGBM to classify the samples into 6
categories for prediction. We found that by combining three types that undergo
long-term double synchronization into one label, the classifier effectively
recognized these features. Apart from systems experiencing long-term double
synchronization, the median relative errors of the predicted evolutionary
curves were all below 4%. Our work provides an efficient method to save
significant computational resources and time with minimal loss in accuracy.
This research also lays the foundation for analyzing the evolutionary
characteristics of systems under different migration states, aiding in the
understanding of the underlying physical mechanisms of such systems. Finally,
to a large extent, our approach could replace the calculations of theoretical
models.</p></br><a href="http://arxiv.org/pdf/2408.17298v1"><h2>Accelerating the discovery of steady-states of planetary interior
  dynamics with machine learning</h2></a>Authors:  Siddhant Agarwal, Nicola Tosi, Christian HÃ¼ttig, David S. Greenberg, Ali Can Bekar</br>Comments: No comment found</br>Primary Category: physics.flu-dyn</br>All Categories: physics.flu-dyn, astro-ph.EP, cs.AI, cs.LG</br><p>Simulating mantle convection often requires reaching a computationally
expensive steady-state, crucial for deriving scaling laws for thermal and
dynamical flow properties and benchmarking numerical solutions. The strong
temperature dependence of the rheology of mantle rocks causes viscosity
variations of several orders of magnitude, leading to a slow-evolving stagnant
lid where heat conduction dominates, overlying a rapidly-evolving and strongly
convecting region. Time-stepping methods, while effective for fluids with
constant viscosity, are hindered by the Courant criterion, which restricts the
time step based on the system's maximum velocity and grid size. Consequently,
achieving steady-state requires a large number of time steps due to the
disparate time scales governing the stagnant and convecting regions.
  We present a concept for accelerating mantle convection simulations using
machine learning. We generate a dataset of 128 two-dimensional simulations with
mixed basal and internal heating, and pressure- and temperature-dependent
viscosity. We train a feedforward neural network on 97 simulations to predict
steady-state temperature profiles. These can then be used to initialize
numerical time stepping methods for different simulation parameters. Compared
to typical initializations, the number of time steps required to reach
steady-state is reduced by a median factor of 3.75. The benefit of this method
lies in requiring very few simulations to train on, providing a solution with
no prediction error as we initialize a numerical method, and posing minimal
computational overhead at inference time. We demonstrate the effectiveness of
our approach and discuss the potential implications for accelerated simulations
for advancing mantle convection research.</p></br>
