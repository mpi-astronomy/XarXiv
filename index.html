search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202507102000+TO+202507162000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, physics.data-an, stat.*, cs.AI staritng 202507102000 and ending 202507162000</h1>Feed last updated: 2025-07-16T00:00:00-04:00<a href="http://arxiv.org/pdf/2507.10738v1"><h2>Information Field Theory based Event Reconstruction for Cosmic Ray Radio
  Detectors</h2></a>Authors:  Simon Strähnz, Tim Huege, Torsten Enßlin, Karen Terveer, Anna Nelles</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.HE, physics.data-an</br><p>Detection of extensive air showers with radio antennas is an appealing
technique in cosmic ray physics. However, because of the high level of
measurement noise, current reconstruction methods still leave room for
improvement. Furthermore, reconstruction efforts typically focus only on a
single aspect of the signal, such as the energy fluence or arrival time.
Bayesian inference is then a natural choice for a holistic approach to
reconstruction, yet, this problem would be ill-posed, since the electric field
is a continuous quantity. Information Field Theory provides the solution for
this by providing a statistical framework to deal with discretised fields in
the continuum limit. We are currently developing models for this novel approach
to reconstructing extensive air showers. The model described here is based on
the best current understanding of the emission mechanisms: It uses
parametrisations of the lateral signal strength distribution, charge-excess
contribution and spectral shape. Shower-to-shower fluctuations and narrowband
RFI are modelled using Gaussian processes. Combined with a detailed detector
description, this model can infer not only the electric field, but also the
shower geometry, electromagnetic energy and position of shower maximum. Another
big achievement of this approach is its ability to naturally provide
uncertainties for the reconstruction, which has been shown to be difficult in
more traditional methods. With such an open framework and robust computational
methods based in Information Field Theory, it will also be easy to incorporate
new insights and additional data, such as timing distributions or particle
detector data, in the future. This approach has a high potential to exploit the
full information content of a complex detector with rigorous statistical
methods, in a way that directly includes domain knowledge.</p></br><a href="http://arxiv.org/pdf/2507.11192v1"><h2>Recent Advances in Simulation-based Inference for Gravitational Wave
  Data Analysis</h2></a>Authors:  Bo Liang, He Wang</br>Comments: 30 pages, 6 figures, 1 table. Published version accepted by
  Astronomical Techniques and Instruments (ATI)</br>Primary Category: gr-qc</br>All Categories: gr-qc, astro-ph.HE, astro-ph.IM, cs.LG, stat.ML</br><p>The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration
has ushered in a new era of observational astronomy, emphasizing the need for
rapid and detailed parameter estimation and population-level analyses.
Traditional Bayesian inference methods, particularly Markov chain Monte Carlo,
face significant computational challenges when dealing with the
high-dimensional parameter spaces and complex noise characteristics inherent in
gravitational wave data. This review examines the emerging role of
simulation-based inference methods in gravitational wave astronomy, with a
focus on approaches that leverage machine-learning techniques such as
normalizing flows and neural posterior estimation. We provide a comprehensive
overview of the theoretical foundations underlying various simulation-based
inference methods, including neural posterior estimation, neural ratio
estimation, neural likelihood estimation, flow matching, and consistency
models. We explore the applications of these methods across diverse
gravitational wave data processing scenarios, from single-source parameter
estimation and overlapping signal analysis to testing general relativity and
conducting population studies. Although these techniques demonstrate speed
improvements over traditional methods in controlled studies, their
model-dependent nature and sensitivity to prior assumptions are barriers to
their widespread adoption. Their accuracy, which is similar to that of
conventional methods, requires further validation across broader parameter
spaces and noise conditions.</p></br>
