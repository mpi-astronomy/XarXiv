search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202411092000+TO+202411152000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.AI, stat.*, physics.data-an, cs.LG staritng 202411092000 and ending 202411152000</h1>Feed last updated: 2024-11-14T00:00:00-05:00<a href="http://arxiv.org/pdf/2411.09311v1"><h2>Compression Method for Solar Polarization Spectra Collected from Hinode
  SOT/SP Observations</h2></a>Authors:  Jargalmaa Batmunkh, Yusuke Iida, Takayoshi Oba, Haruhisa Iijima</br>Comments: No comment found</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, astro-ph.SR</br><p>The complex structure and extensive details of solar spectral data, combined
with a recent surge in volume, present significant processing challenges. To
address this, we propose a deep learning-based compression technique using deep
autoencoder (DAE) and 1D-convolutional autoencoder (CAE) models developed with
Hinode SOT/SP data. We focused on compressing Stokes I and V polarization
spectra from the quiet Sun, as well as from active regions, providing a novel
insight into comprehensive spectral analysis by incorporating spectra from
extreme magnetic fields. The results indicate that the CAE model outperforms
the DAE model in reconstructing Stokes profiles, demonstrating greater
robustness and achieving reconstruction errors around the observational noise
level. The proposed method has proven effective in compressing Stokes I and V
spectra from both the quiet Sun and active regions, highlighting its potential
for impactful applications in solar spectral analysis, such as detection of
unusual spectral signals.</p></br><a href="http://arxiv.org/pdf/2411.08842v1"><h2>AstroM$^3$: A self-supervised multimodal model for astronomy</h2></a>Authors:  Mariia Rizhko, Joshua S. Bloom</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI</br><p>While machine-learned models are now routinely employed to facilitate
astronomical inquiry, model inputs tend to be limited to a primary data source
(namely images or time series) and, in the more advanced approaches, some
metadata. Yet with the growing use of wide-field, multiplexed observational
resources, individual sources of interest often have a broad range of
observational modes available. Here we construct an astronomical multimodal
dataset and propose AstroM$^3$, a self-supervised pre-training approach that
enables a model to learn from multiple modalities simultaneously. Specifically,
we extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodal
setting, allowing the integration of time-series photometry data, spectra, and
astrophysical metadata. In a fine-tuning supervised setting, our results
demonstrate that CLIP pre-training improves classification performance for
time-series photometry, where accuracy increases from 84.6% to 91.5%.
Furthermore, CLIP boosts classification accuracy by up to 12.6% when the
availability of labeled data is limited, showing the effectiveness of
leveraging larger corpora of unlabeled data. In addition to fine-tuned
classification, we can use the trained model in other downstream tasks that are
not explicitly contemplated during the construction of the self-supervised
model. In particular we show the efficacy of using the learned embeddings for
misclassifications identification, similarity search, and anomaly detection.
One surprising highlight is the "rediscovery" of Mira subtypes and two
Rotational variable subclasses using manifold learning and dimension reduction
algorithm. To our knowledge this is the first construction of an $n>2$ mode
model in astronomy. Extensions to $n>3$ modes is naturally anticipated with
this approach.</p></br>
