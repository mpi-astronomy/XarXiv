search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202412102000+TO+202412162000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, cs.AI, stat.*, physics.data-an staritng 202412102000 and ending 202412162000</h1>Feed last updated: 2024-12-15T00:00:00-05:00<a href="http://arxiv.org/pdf/2412.09002v1"><h2>Stellar parameter prediction and spectral simulation using machine
  learning</h2></a>Authors:  Vojtěch Cvrček, Martino Romaniello, Radim Šára, Wolfram Freudling, Pascal Ballester</br>Comments: Accepted for publication in Astronomy & Astrophysics</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, astro-ph.EP, astro-ph.IM, cs.LG</br><p>We applied machine learning to the entire data history of ESO's High Accuracy
Radial Velocity Planet Searcher (HARPS) instrument. Our primary goal was to
recover the physical properties of the observed objects, with a secondary
emphasis on simulating spectra. We systematically investigated the impact of
various factors on the accuracy and fidelity of the results, including the use
of simulated data, the effect of varying amounts of real training data, network
architectures, and learning paradigms. Our approach integrates supervised and
unsupervised learning techniques within autoencoder frameworks. Our methodology
leverages an existing simulation model that utilizes a library of existing
stellar spectra in which the emerging flux is computed from first principles
rooted in physics and a HARPS instrument model to generate simulated spectra
comparable to observational data. We trained standard and variational
autoencoders on HARPS data to predict spectral parameters and generate spectra.
Our models excel at predicting spectral parameters and compressing real
spectra, and they achieved a mean prediction error of approximately 50 K for
effective temperatures, making them relevant for most astrophysical
applications. Furthermore, the models predict metallicity ([M/H]) and surface
gravity (log g) with an accuracy of approximately 0.03 dex and 0.04 dex,
respectively, underscoring their broad applicability in astrophysical research.
The models' computational efficiency, with processing times of 779.6 ms on CPU
and 3.97 ms on GPU, makes them valuable for high-throughput applications like
massive spectroscopic surveys and large archival studies. By achieving accuracy
comparable to classical methods with significantly reduced computation time,
our methodology enhances the scope and efficiency of spectroscopic analysis.</p></br><a href="http://arxiv.org/pdf/2412.10093v1"><h2>AI in the Cosmos</h2></a>Authors:  N. Sahakyan</br>Comments: In press in the International Journal of Modern Physics D; invited
  talk at the 17th Marcel Grossmann Meeting</br>Primary Category: astro-ph.HE</br>All Categories: astro-ph.HE, astro-ph.GA, astro-ph.IM, cs.AI</br><p>Artificial intelligence (AI) is revolutionizing research by enabling the
efficient analysis of large datasets and the discovery of hidden patterns. In
astrophysics, AI has become essential, transforming the classification of
celestial sources, data modeling, and the interpretation of observations. In
this review, I highlight examples of AI applications in astrophysics, including
source classification, spectral energy distribution modeling, and discuss the
advancements achievable through generative AI. However, the use of AI
introduces challenges, including biases, errors, and the "black box" nature of
AI models, which must be resolved before their application. These issues can be
addressed through the concept of Human-Guided AI (HG-AI), which integrates
human expertise and domain-specific knowledge into AI applications. This
approach aims to ensure that AI is applied in a robust, interpretable, and
ethical manner, leading to deeper insights and fostering scientific excellence.</p></br><a href="http://arxiv.org/pdf/2412.08589v1"><h2>SPACE-SUIT: An Artificial Intelligence based chromospheric feature
  extractor and classifier for SUIT</h2></a>Authors:  Pranava Seth, Vishal Upendran, Megha Anand, Janmejoy Sarkar, Soumya Roy, Priyadarshan Chaki, Pratyay Chowdhury, Borishan Ghosh, Durgesh Tripathi</br>Comments: No comment found</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, astro-ph.IM, cs.CV, cs.LG</br><p>The Solar Ultraviolet Imaging Telescope(SUIT) onboard Aditya-L1 is an imager
that observes the solar photosphere and chromosphere through observations in
the wavelength range of 200-400 nm. A comprehensive understanding of the plasma
and thermodynamic properties of chromospheric and photospheric morphological
structures requires a large sample statistical study, necessitating the
development of automatic feature detection methods. To this end, we develop the
feature detection algorithm SPACE-SUIT: Solar Phenomena Analysis and
Classification using Enhanced vision techniques for SUIT, to detect and
classify the solar chromospheric features to be observed from SUIT's Mg II k
filter. Specifically, we target plage regions, sunspots, filaments, and
off-limb structures. SPACE uses You Only Look Once(YOLO), a neural
network-based model to identify regions of interest. We train and validate
SPACE using mock-SUIT images developed from Interface Region Imaging
Spectrometer(IRIS) full-disk mosaic images in Mg II k line, while we also
perform detection on Level-1 SUIT data. SPACE achieves an approximate precision
of 0.788, recall 0.863 and MAP of 0.874 on the validation mock SUIT FITS
dataset. Given the manual labeling of our dataset, we perform "self-validation"
by applying statistical measures and Tamura features on the ground truth and
predicted bounding boxes. We find the distributions of entropy, contrast,
dissimilarity, and energy to show differences in the features. These
differences are qualitatively captured by the detected regions predicted by
SPACE and validated with the observed SUIT images, even in the absence of
labeled ground truth. This work not only develops a chromospheric feature
extractor but also demonstrates the effectiveness of statistical metrics and
Tamura features for distinguishing chromospheric features, offering independent
validation for future detection schemes.</p></br><a href="http://arxiv.org/pdf/2412.08265v1"><h2>Fast GPU-Powered and Auto-Differentiable Forward Modeling of IFU Data
  Cubes</h2></a>Authors:  Ufuk Çakır, Anna Lena Schaible, Tobias Buck</br>Comments: accepted to the Machine Learning and the Physical Sciences Workshop,
  NeurIPS 2024</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.GA, physics.comp-ph, physics.data-an</br><p>We present RUBIX, a fully tested, well-documented, and modular Open Source
tool developed in JAX, designed to forward model IFU cubes of galaxies from
cosmological hydrodynamical simulations. The code automatically parallelizes
computations across multiple GPUs, demonstrating performance improvements over
state-of-the-art codes by a factor of 600. This optimization reduces compute
times from hours to only seconds. RUBIX leverages JAX's auto-differentiation
capabilities to enable not only forward modeling but also gradient computations
through the entire pipeline paving the way for new methodological approaches
such as e.g. gradient-based optimization of astrophysics model parameters.
RUBIX is open-source and available on GitHub:
https://github.com/ufuk-cakir/rubix.</p></br><a href="http://arxiv.org/pdf/2412.08490v1"><h2>SuperCode: Sustainability PER AI-driven CO-DEsign</h2></a>Authors:  P. Chris Broekema, Rob V. van Nieuwpoort</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI</br><p>Currently, data-intensive scientific applications require vast amounts of
compute resources to deliver world-leading science. The climate emergency has
made it clear that unlimited use of resources (e.g., energy) for scientific
discovery is no longer acceptable. Future computing hardware promises to be
much more energy efficient, but without better optimized software this cannot
reach its full potential. In this vision paper, we propose a generic AI-driven
co-design methodology, using specialized Large Language Models (like ChatGPT),
to effectively generate efficient code for emerging computing hardware. We
describe how we will validate our methodology with two radio astronomy
applications, with sustainability as the key performance indicator. This paper
is a modified version of our accepted SuperCode project proposal. We present it
here in this form to introduce the vision behind this project and to
disseminate the work in the spirit of Open Science and transparency. An
additional aim is to collect feedback, invite potential collaboration partners
and use-cases to join the project.</p></br><a href="http://arxiv.org/pdf/2412.08254v1"><h2>PADÉ FILTERING, Principles and Use: an Introductory Report</h2></a>Authors:  Jean-Daniel Fournier, Mikhaël Pichot du Mézeray</br>Comments: No comment found</br>Primary Category: gr-qc</br>All Categories: gr-qc, astro-ph.IM, physics.data-an</br><p>This report aims to provide gravitational waves data analysts with an
introduction to the ideas and practice of the Pad\'e Filtering method for
disentangling a signal from the noise. Technically it comes to the tracking of
the zeros and singularities of random z-Transforms by noisy Pad\'e
Approximants.</p></br><a href="http://arxiv.org/pdf/2412.08672v1"><h2>Efficient Gravitational Wave Parameter Estimation via Knowledge
  Distillation: A ResNet1D-IAF Approach</h2></a>Authors:  Xihua Zhu, Yiqian Yang, Fan Zhang</br>Comments: 7 pages, 4 figures, 2 tables</br>Primary Category: gr-qc</br>All Categories: gr-qc, astro-ph.IM, cs.LG, physics.data-an, I.2.6</br><p>With the rapid development of gravitational wave astronomy, the increasing
number of detected events necessitates efficient methods for parameter
estimation and model updates. This study presents a novel approach using
knowledge distillation techniques to enhance computational efficiency in
gravitational wave analysis. We develop a framework combining ResNet1D and
Inverse Autoregressive Flow (IAF) architectures, where knowledge from a complex
teacher model is transferred to a lighter student model. Our experimental
results show that the student model achieves a validation loss of 3.70 with
optimal configuration (40,100,0.75), compared to the teacher model's 4.09,
while reducing the number of parameters by 43\%. The Jensen-Shannon divergence
between teacher and student models remains below 0.0001 across network layers,
indicating successful knowledge transfer. By optimizing ResNet layers (7-16)
and hidden features (70-120), we achieve a 35\% reduction in inference time
while maintaining parameter estimation accuracy. This work demonstrates
significant improvements in computational efficiency for gravitational wave
data analysis, providing valuable insights for real-time event processing.</p></br><a href="http://arxiv.org/pdf/2412.09832v1"><h2>Multivariate Time Series Clustering for Environmental State
  Characterization of Ground-Based Gravitational-Wave Detectors</h2></a>Authors:  Rutuja Gurav, Isaac Kelly, Pooyan Goodarzi, Anamaria Effler, Barry Barish, Evangelos Papalexakis, Jonathan Richardson</br>Comments: 8 pages, 6 figures, Accepted to The 5th International Workshop on Big
  Data & AI Tools, Methods, and Use Cases for Innovative Scientific Discovery
  (BTSD 2024)</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, gr-qc</br><p>Gravitational-wave observatories like LIGO are large-scale, terrestrial
instruments housed in infrastructure that spans a multi-kilometer geographic
area and which must be actively controlled to maintain operational stability
for long observation periods. Despite exquisite seismic isolation, they remain
susceptible to seismic noise and other terrestrial disturbances that can couple
undesirable vibrations into the instrumental infrastructure, potentially
leading to control instabilities or noise artifacts in the detector output. It
is, therefore, critical to characterize the seismic state of these
observatories to identify a set of temporal patterns that can inform the
detector operators in day-to-day monitoring and diagnostics. On a day-to-day
basis, the operators monitor several seismically relevant data streams to
diagnose operational instabilities and sources of noise using some simple
empirically-determined thresholds. It can be untenable for a human operator to
monitor multiple data streams in this manual fashion and thus a distillation of
these data-streams into a more human-friendly format is sought. In this paper,
we present an end-to-end machine learning pipeline for features-based
multivariate time series clustering to achieve this goal and to provide
actionable insights to the detector operators by correlating found clusters
with events of interest in the detector.</p></br>
