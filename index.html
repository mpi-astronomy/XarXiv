search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202408312000+TO+202409062000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, cs.LG, physics.data-an, cs.AI staritng 202408312000 and ending 202409062000</h1>Feed last updated: 2024-09-06T00:00:00-04:00<a href="http://arxiv.org/pdf/2409.03466v1"><h2>Panopticon: a novel deep learning model to detect single transit events
  with no prior data filtering in PLATO light curves</h2></a>Authors:  H. G. Vivien, M. Deleuil, N. Jannsen, J. De Ridder, D. Seynaeve, M. -A. Carpine, Y. Zerah</br>Comments: Submitted to A&A</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, astro-ph.IM, cs.LG</br><p>To prepare for the analyses of the future PLATO light curves, we develop a
deep learning model, Panopticon, to detect transits in high precision
photometric light curves. Since PLATO's main objective is the detection of
temperate Earth-size planets around solar-type stars, the code is designed to
detect individual transit events. The filtering step, required by conventional
detection methods, can affect the transit, which could be an issue for long and
shallow transits. To protect transit shape and depth, the code is also designed
to work on unfiltered light curves. We trained the model on a set of simulated
PLATO light curves in which we injected, at pixel level, either planetary,
eclipsing binary, or background eclipsing binary signals. We also include a
variety of noises in our data, such as granulation, stellar spots or cosmic
rays. The approach is able to recover 90% of our test population, including
more than 25% of the Earth-analogs, even in the unfiltered light curves. The
model also recovers the transits irrespective of the orbital period, and is
able to retrieve transits on a unique event basis. These figures are obtained
when accepting a false alarm rate of 1%. When keeping the false alarm rate low
(<0.01%), it is still able to recover more than 85% of the transit signals. Any
transit deeper than 180ppm is essentially guaranteed to be recovered. This
method is able to recover transits on a unique event basis, and does so with a
low false alarm rate. Thanks to light curves being one-dimensional, model
training is fast, on the order of a few hours per model. This speed in training
and inference, coupled to the recovery effectiveness and precision of the model
make it an ideal tool to complement, or be used ahead of, classical approaches.</p></br><a href="http://arxiv.org/pdf/2409.02154v1"><h2>COmoving Computer Acceleration (COCA): $N$-body simulations in an
  emulated frame of reference</h2></a>Authors:  Deaglan J. Bartlett, Marco Chiarenza, Ludvig Doeser, Florent Leclercq</br>Comments: 23 pages, 13 figures</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, cs.LG, stat.ML</br><p>$N$-body simulations are computationally expensive, so machine-learning
(ML)-based emulation techniques have emerged as a way to increase their speed.
Although fast, surrogate models have limited trustworthiness due to potentially
substantial emulation errors that current approaches cannot correct for. To
alleviate this problem, we introduce COmoving Computer Acceleration (COCA), a
hybrid framework interfacing ML with an $N$-body simulator. The correct
physical equations of motion are solved in an emulated frame of reference, so
that any emulation error is corrected by design. This approach corresponds to
solving for the perturbation of particle trajectories around the machine-learnt
solution, which is computationally cheaper than obtaining the full solution,
yet is guaranteed to converge to the truth as one increases the number of force
evaluations. Although applicable to any ML algorithm and $N$-body simulator,
this approach is assessed in the particular case of particle-mesh cosmological
simulations in a frame of reference predicted by a convolutional neural
network, where the time dependence is encoded as an additional input parameter
to the network. COCA efficiently reduces emulation errors in particle
trajectories, requiring far fewer force evaluations than running the
corresponding simulation without ML. We obtain accurate final density and
velocity fields for a reduced computational budget. We demonstrate that this
method shows robustness when applied to examples outside the range of the
training data. When compared to the direct emulation of the Lagrangian
displacement field using the same training resources, COCA's ability to correct
emulation errors results in more accurate predictions. COCA makes $N$-body
simulations cheaper by skipping unnecessary force evaluations, while still
solving the correct equations of motion and correcting for emulation errors
made by ML.</p></br><a href="http://arxiv.org/pdf/2409.01825v1"><h2>AstroMAE: Redshift Prediction Using a Masked Autoencoder with a Novel
  Fine-Tuning Architecture</h2></a>Authors:  Amirreza Dolatpour Fathkouhi, Geoffrey Charles Fox</br>Comments: This paper has been accepted to 2024 IEEE 20th International
  Conference on e-Science</br>Primary Category: cs.CV</br>All Categories: cs.CV, astro-ph.GA, astro-ph.IM, cs.CE, cs.LG</br><p>Redshift prediction is a fundamental task in astronomy, essential for
understanding the expansion of the universe and determining the distances of
astronomical objects. Accurate redshift prediction plays a crucial role in
advancing our knowledge of the cosmos. Machine learning (ML) methods, renowned
for their precision and speed, offer promising solutions for this complex task.
However, traditional ML algorithms heavily depend on labeled data and
task-specific feature extraction. To overcome these limitations, we introduce
AstroMAE, an innovative approach that pretrains a vision transformer encoder
using a masked autoencoder method on Sloan Digital Sky Survey (SDSS) images.
This technique enables the encoder to capture the global patterns within the
data without relying on labels. To the best of our knowledge, AstroMAE
represents the first application of a masked autoencoder to astronomical data.
By ignoring labels during the pretraining phase, the encoder gathers a general
understanding of the data. The pretrained encoder is subsequently fine-tuned
within a specialized architecture tailored for redshift prediction. We evaluate
our model against various vision transformer architectures and CNN-based
models, demonstrating the superior performance of AstroMAEs pretrained model
and fine-tuning architecture.</p></br><a href="http://arxiv.org/pdf/2409.02980v1"><h2>How DREAMS are made: Emulating Satellite Galaxy and Subhalo Populations
  with Diffusion Models and Point Clouds</h2></a>Authors:  Tri Nguyen, Francisco Villaescusa-Navarro, Siddharth Mishra-Sharma, Carolina Cuesta-Lazaro, Paul Torrey, Arya Farahi, Alex M. Garcia, Jonah C. Rose, Stephanie O'Neil, Mark Vogelsberger, Xuejian Shen, Cian Roche, Daniel Anglés-Alcázar, Nitya Kallivayalil, Julian B. Muñoz, Francis-Yan Cyr-Racine, Sandip Roy, Lina Necib, Kassidy E. Kollmann</br>Comments: Submitted to ApJ; 30 + 6 pages; 11 + 4 figures; Comments welcomed</br>Primary Category: astro-ph.GA</br>All Categories: astro-ph.GA, astro-ph.CO, cs.LG</br><p>The connection between galaxies and their host dark matter (DM) halos is
critical to our understanding of cosmology, galaxy formation, and DM physics.
To maximize the return of upcoming cosmological surveys, we need an accurate
way to model this complex relationship. Many techniques have been developed to
model this connection, from Halo Occupation Distribution (HOD) to empirical and
semi-analytic models to hydrodynamic. Hydrodynamic simulations can incorporate
more detailed astrophysical processes but are computationally expensive; HODs,
on the other hand, are computationally cheap but have limited accuracy. In this
work, we present NeHOD, a generative framework based on variational diffusion
model and Transformer, for painting galaxies/subhalos on top of DM with an
accuracy of hydrodynamic simulations but at a computational cost similar to
HOD. By modeling galaxies/subhalos as point clouds, instead of binning or
voxelization, we can resolve small spatial scales down to the resolution of the
simulations. For each halo, NeHOD predicts the positions, velocities, masses,
and concentrations of its central and satellite galaxies. We train NeHOD on the
TNG-Warm DM suite of the DREAMS project, which consists of 1024 high-resolution
zoom-in hydrodynamic simulations of Milky Way-mass halos with varying warm DM
mass and astrophysical parameters. We show that our model captures the complex
relationships between subhalo properties as a function of the simulation
parameters, including the mass functions, stellar-halo mass relations,
concentration-mass relations, and spatial clustering. Our method can be used
for a large variety of downstream applications, from galaxy clustering to
strong lensing studies.</p></br><a href="http://arxiv.org/pdf/2409.02150v1"><h2>Hazardous Asteroids Classification</h2></a>Authors:  Thai Duy Quy, Alvin Buana, Josh Lee, Rakha Asyrofi</br>Comments: 6 pages</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, cs.LG</br><p>Hazardous asteroid has been one of the concerns for humankind as fallen
asteroid on earth could cost a huge impact on the society.Monitoring these
objects could help predict future impact events, but such efforts are hindered
by the large numbers of objects that pass in the Earth's vicinity. The aim of
this project is to use machine learning and deep learning to accurately
classify hazardous asteroids. A total of ten methods which consist of five
machine learning algorithms and five deep learning models are trained and
evaluated to find the suitable model that solves the issue. We experiment on
two datasets, one from Kaggle and one we extracted from a web service called
NeoWS which is a RESTful web service from NASA that provides information about
near earth asteroids, it updates every day. In overall, the model is tested on
two datasets with different features to find the most accurate model to perform
the classification.</p></br><a href="http://arxiv.org/pdf/2409.01614v1"><h2>On-chain Validation of Tracking Data Messages (TDM) Using Distributed
  Deep Learning on a Proof of Stake (PoS) Blockchain</h2></a>Authors:  Yasir Latif, Anirban Chowdhury, Samya Bagchi</br>Comments: Accepted for AMOS 2024</br>Primary Category: cs.CR</br>All Categories: cs.CR, astro-ph.EP, cs.LG</br><p>Trustless tracking of Resident Space Objects (RSOs) is crucial for Space
Situational Awareness (SSA), especially during adverse situations. The
importance of transparent SSA cannot be overstated, as it is vital for ensuring
space safety and security. In an era where RSO location information can be
easily manipulated, the risk of RSOs being used as weapons is a growing
concern. The Tracking Data Message (TDM) is a standardized format for
broadcasting RSO observations. However, the varying quality of observations
from diverse sensors poses challenges to SSA reliability. While many countries
operate space assets, relatively few have SSA capabilities, making it crucial
to ensure the accuracy and reliability of the data. Current practices assume
complete trust in the transmitting party, leaving SSA capabilities vulnerable
to adversarial actions such as spoofing TDMs. This work introduces a trustless
mechanism for TDM validation and verification using deep learning over
blockchain. By leveraging the trustless nature of blockchain, our approach
eliminates the need for a central authority, establishing consensus-based
truth. We propose a state-of-the-art, transformer-based orbit propagator that
outperforms traditional methods like SGP4, enabling cross-validation of
multiple observations for a single RSO. This deep learning-based transformer
model can be distributed over a blockchain, allowing interested parties to host
a node that contains a part of the distributed deep learning model. Our system
comprises decentralised observers and validators within a Proof of Stake (PoS)
blockchain. Observers contribute TDM data along with a stake to ensure honesty,
while validators run the propagation and validation algorithms. The system
rewards observers for contributing verified TDMs and penalizes those submitting
unverifiable data.</p></br><a href="http://arxiv.org/pdf/2409.01407v1"><h2>$\mathtt{emuflow}$: Normalising Flows for Joint Cosmological Analysis</h2></a>Authors:  Arrykrishna Mootoovaloo, Carlos García-García, David Alonso, Jaime Ruiz-Zapatero</br>Comments: 13 pages, 5 figures</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, cs.LG</br><p>Given the growth in the variety and precision of astronomical datasets of
interest for cosmology, the best cosmological constraints are invariably
obtained by combining data from different experiments. At the likelihood level,
one complication in doing so is the need to marginalise over large-dimensional
parameter models describing the data of each experiment. These include both the
relatively small number of cosmological parameters of interest and a large
number of "nuisance" parameters. Sampling over the joint parameter space for
multiple experiments can thus become a very computationally expensive
operation. This can be significantly simplified if one could sample directly
from the marginal cosmological posterior distribution of preceding experiments,
depending only on the common set of cosmological parameters. In this paper, we
show that this can be achieved by emulating marginal posterior distributions
via normalising flows. The resulting trained normalising flow models can be
used to efficiently combine cosmological constraints from independent datasets
without increasing the dimensionality of the parameter space under study. We
show that the method is able to accurately describe the posterior distribution
of real cosmological datasets, as well as the joint distribution of different
datasets, even when significant tension exists between experiments. The
resulting joint constraints can be obtained in a fraction of the time it would
take to combine the same datasets at the level of their likelihoods. We
construct normalising flow models for a set of public cosmological datasets of
general interests and make them available, together with the software used to
train them, and to exploit them in cosmological parameter inference.</p></br>
