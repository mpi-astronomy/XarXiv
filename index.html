search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202512032000+TO+202512092000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on physics.data-an, cs.AI, cs.LG, stat.* staritng 202512032000 and ending 202512092000</h1>Feed last updated: 2025-12-09T04:20:14Z<a href="https://arxiv.org/pdf/2512.04803v1"><h2>287,872 Supermassive Black Holes Masses: Deep Learning Approaching Reverberation Mapping Accuracy</h2></a>Authors:  Yuhao Lu, HengJian SiTu, Jie Li, Yixuan Li, Yang Liu, Wenbin Lin, Yu Wang</br>Comments: 14 pages, 9 figures. Submitted to Journal of High Energy Astrophysics</br>Primary Category: astro-ph.GA</br>All Categories: astro-ph.GA, astro-ph.HE, astro-ph.IM, cs.AI</br><p>We present a population-scale catalogue of 287,872 supermassive black hole masses with high accuracy. Using a deep encoder-decoder network trained on optical spectra with reverberation-mapping (RM) based labels of 849 quasars and applied to all SDSS quasars up to $z=4$, our method achieves a root-mean-square error of $0.058$\,dex, a relative uncertainty of $\approx 14\%$, and coefficient of determination $R^{2}\approx0.91$ with respect to RM-based masses, far surpassing traditional single-line virial estimators. Notably, the high accuracy is maintained for both low ($<10^{7.5}\,M_\odot$) and high ($>10^{9}\,M_\odot$) mass quasars, where empirical relations are unreliable.</p></br><a href="https://arxiv.org/pdf/2512.06086v1"><h2>Generalized tension metrics for multiple cosmological datasets</h2></a>Authors:  Matías Leizerovich, Susana J. Landau, Claudia G. Scóccola</br>Comments: 6 pages, 5 figures, 2 tables</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, astro-ph.IM, hep-ex, hep-ph, physics.data-an</br><p>We introduce a novel estimator to quantify statistical tensions among multiple cosmological datasets simultaneously. This estimator generalizes the Difference-in-Means statistic, $Q_{\rm DM}$, to the multi-dataset regime. Our framework enables the detection of dominant tension directions in the shared parameter space. It further provides a geometric interpretation of the tension for the two- and three-dataset cases in two dimensions. According to this approach, the previously reported increase in tension between DESI and Planck from $1.9σ$ (DR1) to $2.3σ$(DR2) is reinterpreted as a more modest shift from $1.18σ^{\rm eff}$ (DR1) to $1.45σ^{\rm eff}$ (DR2). These new tools may also prove valuable across research fields where dataset discrepancies arise.</p></br><a href="https://arxiv.org/pdf/2512.04600v1"><h2>The dynamical memory of tidal stellar streams: Joint inference of the Galactic potential and the progenitor of GD-1 with flow matching</h2></a>Authors:  Giuseppe Viterbo, Tobias Buck</br>Comments: submitted to A&A, comments welcome, all source code to reproduce this work can be found on GitHub under the url: https://github.com/vepe99/sbi-sim/tree/odisseo_branch . The simulator \textsc{\texttt{Odisseo}} is available on GitHub at the following url: https://github.com/vepe99/Odisseo</br>Primary Category: astro-ph.GA</br>All Categories: astro-ph.GA, physics.class-ph, physics.data-an, physics.space-ph</br><p>Stellar streams offer one of the most sensitive probes of the Milky Way`s gravitational potential, as their phase-space morphology encodes both the tidal field of the host galaxy and the internal structure of their progenitors. In this work, we introduce a framework that leverages Flow Matching and Simulation-Based Inference (SBI) to jointly infer the parameters of the GD-1 progenitor and the global properties of the Milky Way potential. Our aim is to move beyond traditional techniques (e.g. orbit-fitting and action-angle methods) by constructing a fully Bayesian, likelihood-free posterior over both host-galaxy parameters and progenitor properties, thereby capturing the intrinsic coupling between tidal stripping dynamics and the underlying potential. To achieve this, we generate a large suite of mock GD-1-like streams using our differentiable N-body code \textsc{\texttt{Odisseo}}, sampling self-consistent initial conditions from a Plummer sphere and evolving them in a flexible Milky Way potential model. We then apply conditional Flow Matching to learn the vector field that transports a base Gaussian distribution into the posterior, enabling efficient, amortized inference directly from stream phase-space data. We demonstrate that our method successfully recovers the true parameters of a fiducial GD-1 simulation, producing well-calibrated posteriors and accurately reproducing parameter degeneracies arising from progenitor-host interactions. Flow Matching provides a powerful, flexible framework for Galactic Archaeology. Our approach enables joint inference on progenitor and Galactic parameters, capturing complex dependencies that are difficult to model with classical likelihood-based methods.</p></br><a href="https://arxiv.org/pdf/2512.05751v1"><h2>Exoplanet formation inference using conditional invertible neural networks</h2></a>Authors:  Remo Burn, Victor F. Ksoll, Hubert Klahr, Thomas Henning</br>Comments: 10 pages, accepted poster for the Machine Learning and the Physical Sciences Workshop at the 39th conference on Neural Information Processing Systems (NeurIPS 2025)</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, cs.NE, physics.data-an</br><p>The interpretation of the origin of observed exoplanets is usually done only qualitatively due to uncertainties of key parameters in planet formation models. To allow a quantitative methodology which traces back in time to the planet birth locations, we train recently developed conditional invertible neural networks (cINN) on synthetic data from a global planet formation model which tracks growth from dust grains to evolved final giant planets. In addition to deterministic single planet formation runs, we also include gravitationally interacting planets in multiplanetary systems, which include some measure of chaos. For the latter case, we treat them as individual planets or choose the two or three planets most likely to be discovered by telescopes. We find that training on multiplanetary data, each planet treated as individual point, is promising. The single-planet data only covers a small range of planets and does not extrapolate well to planet properties not included in the training data. Extension to planetary systems will require more training data due to the higher dimensionality of the problem.</p></br><a href="https://arxiv.org/pdf/2512.06642v1"><h2>Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</h2></a>Authors:  Achmad Ardani Prasha, Clavino Ourizqi Rachmadi, Muhamad Fauzan Ibnu Syahlan, Naufal Rahfi Anugerah, Nanda Garin Raditya, Putri Amelia, Sabrina Laila Mutiara, Hilman Syachr Ramadhan</br>Comments: 21 pages, 7 figures, 3 table</br>Primary Category: cs.CV</br>All Categories: cs.CV, astro-ph.CO, astro-ph.IM, cs.AI, cs.LG</br><p>Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.</p></br>
