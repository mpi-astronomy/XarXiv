search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202407252000+TO+202407312000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, physics.data-an, cs.AI, cs.LG staritng 202407252000 and ending 202407312000</h1>Feed last updated: 2024-07-31T00:00:00-04:00<a href="http://arxiv.org/pdf/2407.19481v1"><h2>What can we learn about Reionization astrophysical parameters using
  Gaussian Process Regression?</h2></a>Authors:  Purba Mukherjee, Antara Dey, Supratik Pal</br>Comments: No comment found</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, astro-ph.IM, cs.LG</br><p>Reionization is one of the least understood processes in the evolution
history of the Universe, mostly because of the numerous astrophysical processes
occurring simultaneously about which we do not have a very clear idea so far.
In this article, we use the Gaussian Process Regression (GPR) method to learn
the reionization history and infer the astrophysical parameters. We reconstruct
the UV luminosity density function using the HFF and early JWST data. From the
reconstructed history of reionization, the global differential brightness
temperature fluctuation during this epoch has been computed. We perform MCMC
analysis of the global 21-cm signal using the instrumental specifications of
SARAS, in combination with Lyman-$\alpha$ ionization fraction data, Planck
optical depth measurements and UV luminosity data. Our analysis reveals that
GPR can help infer the astrophysical parameters in a model-agnostic way than
conventional methods. Additionally, we analyze the 21-cm power spectrum using
the reconstructed history of reionization and demonstrate how the future 21-cm
mission SKA, in combination with Planck and Lyman-$\alpha$ forest data,
improves the bounds on the reionization astrophysical parameters by doing a
joint MCMC analysis for the astrophysical parameters plus 6 cosmological
parameters for $\Lambda$CDM model. The results make the GPR-based
reconstruction technique a robust learning process and the inferences on the
astrophysical parameters obtained therefrom are quite reliable that can be used
for future analysis.</p></br><a href="http://arxiv.org/pdf/2407.19048v1"><h2>Rapid Likelihood Free Inference of Compact Binary Coalescences using
  Accelerated Hardware</h2></a>Authors:  Deep Chatterjee, Ethan Marx, William Benoit, Ravi Kumar, Malina Desai, Ekaterina Govorkova, Alec Gunny, Eric Moreno, Rafia Omer, Ryan Raikman, Muhammed Saleem, Shrey Aggarwal, Michael W. Coughlin, Philip Harris, Erik Katsavounidis</br>Comments: Submitted to MLST</br>Primary Category: gr-qc</br>All Categories: gr-qc, astro-ph.IM, cs.LG</br><p>We report a gravitational-wave parameter estimation algorithm, AMPLFI, based
on likelihood-free inference using normalizing flows. The focus of AMPLFI is to
perform real-time parameter estimation for candidates detected by
machine-learning based compact binary coalescence search, Aframe. We present
details of our algorithm and optimizations done related to data-loading and
pre-processing on accelerated hardware. We train our model using binary
black-hole (BBH) simulations on real LIGO-Virgo detector noise. Our model has
$\sim 6$ million trainable parameters with training times $\lesssim 24$ hours.
Based on online deployment on a mock data stream of LIGO-Virgo data, Aframe +
AMPLFI is able to pick up BBH candidates and infer parameters for real-time
alerts from data acquisition with a net latency of $\sim 6$s.</p></br><a href="http://arxiv.org/pdf/2407.20432v1"><h2>Neural Surrogate HMC: Accelerated Hamiltonian Monte Carlo with a Neural
  Network Surrogate Likelihood</h2></a>Authors:  Linnea M Wolniewicz, Peter Sadowski, Claudio Corti</br>Comments: 5 pages, 3 figures, accepted at SPAICE Conference 2024</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.HE, I.2.1</br><p>Bayesian Inference with Markov Chain Monte Carlo requires efficient
computation of the likelihood function. In some scientific applications, the
likelihood must be computed by numerically solving a partial differential
equation, which can be prohibitively expensive. We demonstrate that some such
problems can be made tractable by amortizing the computation with a surrogate
likelihood function implemented by a neural network. We show that this has two
additional benefits: reducing noise in the likelihood evaluations and providing
fast gradient calculations. In experiments, the approach is applied to a model
of heliospheric transport of galactic cosmic rays, where it enables efficient
sampling from the posterior of latent parameters in the Parker equation.</p></br><a href="http://arxiv.org/pdf/2407.18647v1"><h2>Towards unveiling the large-scale nature of gravity with the wavelet
  scattering transform</h2></a>Authors:  Georgios Valogiannis, Francisco Villaescusa-Navarro, Marco Baldi</br>Comments: 19 pages, 15 figures, 1 table</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, gr-qc, hep-ph, physics.data-an</br><p>We present the first application of the Wavelet Scattering Transform (WST) in
order to constrain the nature of gravity using the three-dimensional (3D)
large-scale structure of the universe. Utilizing the Quijote-MG N-body
simulations, we can reliably model the 3D matter overdensity field for the f(R)
Hu-Sawicki modified gravity (MG) model down to $k_{\rm max}=0.5$ h/Mpc.
Combining these simulations with the Quijote $\nu$CDM collection, we then
conduct a Fisher forecast of the marginalized constraints obtained on gravity
using the WST coefficients and the matter power spectrum at redshift z=0. Our
results demonstrate that the WST substantially improves upon the 1$\sigma$
error obtained on the parameter that captures deviations from standard General
Relativity (GR), yielding a tenfold improvement compared to the corresponding
matter power spectrum result. At the same time, the WST also enhances the
precision on the $\Lambda$CDM parameters and the sum of neutrino masses, by
factors of 1.2-3.4 compared to the matter power spectrum, respectively. Despite
the overall reduction in the WST performance when we focus on larger scales, it
still provides a relatively $4.5\times$ tighter 1$\sigma$ error for the MG
parameter at $k_{\rm max}=0.2$ h/Mpc, highlighting its great sensitivity to the
underlying gravity theory. This first proof-of-concept study reaffirms the
constraining properties of the WST technique and paves the way for exciting
future applications in order to perform precise large-scale tests of gravity
with the new generation of cutting-edge cosmological data.</p></br><a href="http://arxiv.org/pdf/2407.21008v1"><h2>Bayesian technique to combine independently-trained Machine-Learning
  models applied to direct dark matter detection</h2></a>Authors:  David Cerdeno, Martin de los Rios, Andres D. Perez</br>Comments: 25 pages, 7 figures, 2 tables</br>Primary Category: hep-ph</br>All Categories: hep-ph, astro-ph.CO, physics.data-an</br><p>We carry out a Bayesian analysis of dark matter (DM) direct detection data to
determine particle model parameters using the Truncated Marginal Neural Ratio
Estimation (TMNRE) machine learning technique. TMNRE avoids an explicit
calculation of the likelihood, which instead is estimated from simulated data,
unlike in traditional Markov Chain Monte Carlo (MCMC) algorithms. This
considerably speeds up, by several orders of magnitude, the computation of the
posterior distributions, which allows to perform the Bayesian analysis of an
otherwise computationally prohibitive number of benchmark points. In this
article we demonstrate that, in the TMNRE framework, it is possible to include,
combine, and remove different datasets in a modular fashion, which is fast and
simple as there is no need to re-train the machine learning algorithm or to
define a combined likelihood. In order to assess the performance of this
method, we consider the case of WIMP DM with spin-dependent and independent
interactions with protons and neutrons in a xenon experiment. After validating
our results with MCMC, we employ the TMNRE procedure to determine the regions
where the DM parameters can be reconstructed. Finally, we present CADDENA, a
Python package that implements the modular Bayesian analysis of direct
detection experiments described in this work.</p></br><a href="http://arxiv.org/pdf/2407.18909v1"><h2>Hybrid summary statistics: neural weak lensing inference beyond the
  power spectrum</h2></a>Authors:  T. Lucas Makinen, Alan Heavens, Natalia Porqueres, Tom Charnock, Axel Lapel, Benjamin D. Wandelt</br>Comments: 16 pages, 11 figures. Submitted to JCAP. We provide publicly
  available code at https://github.com/tlmakinen/hybridStatsWL</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, cs.LG, physics.comp-ph, stat.ML, stat.OT</br><p>In inference problems, we often have domain knowledge which allows us to
define summary statistics that capture most of the information content in a
dataset. In this paper, we present a hybrid approach, where such physics-based
summaries are augmented by a set of compressed neural summary statistics that
are optimised to extract the extra information that is not captured by the
predefined summaries. The resulting statistics are very powerful inputs to
simulation-based or implicit inference of model parameters. We apply this
generalisation of Information Maximising Neural Networks (IMNNs) to parameter
constraints from tomographic weak gravitational lensing convergence maps to
find summary statistics that are explicitly optimised to complement angular
power spectrum estimates. We study several dark matter simulation resolutions
in low- and high-noise regimes. We show that i) the information-update
formalism extracts at least $3\times$ and up to $8\times$ as much information
as the angular power spectrum in all noise regimes, ii) the network summaries
are highly complementary to existing 2-point summaries, and iii) our formalism
allows for networks with smaller, physically-informed architectures to match
much larger regression networks with far fewer simulations needed to obtain
asymptotically optimal inference.</p></br>
