search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202407202000+TO+202407262000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.AI, cs.LG, stat.*, physics.data-an staritng 202407202000 and ending 202407262000</h1>Feed last updated: 2024-07-26T00:00:00-04:00<a href="http://arxiv.org/pdf/2407.16917v1"><h2>TelescopeML -- I. An End-to-End Python Package for Interpreting
  Telescope Datasets through Training Machine Learning Models, Generating
  Statistical Reports, and Visualizing Results</h2></a>Authors:  Ehsan, Gharib-Nezhad, Natasha E. Batalha, Hamed Valizadegan, Miguel J. S. Martinho, Mahdi Habibi, Gopal Nookula</br>Comments: Please find the accepted paper with complete reference list at
  https://joss.theoj.org/papers/10.21105/joss.06346</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.EP, cs.LG</br><p>We are on the verge of a revolutionary era in space exploration, thanks to
advancements in telescopes such as the James Webb Space Telescope
(\textit{JWST}). High-resolution, high signal-to-noise spectra from exoplanet
and brown dwarf atmospheres have been collected over the past few decades,
requiring the development of accurate and reliable pipelines and tools for
their analysis. Accurately and swiftly determining the spectroscopic parameters
from the observational spectra of these objects is crucial for understanding
their atmospheric composition and guiding future follow-up observations.
\texttt{TelescopeML} is a Python package developed to perform three main tasks:
1. Process the synthetic astronomical datasets for training a CNN model and
prepare the observational dataset for later use for prediction; 2. Train a CNN
model by implementing the optimal hyperparameters; and 3. Deploy the trained
CNN models on the actual observational data to derive the output spectroscopic
parameters.</p></br><a href="http://arxiv.org/pdf/2407.17667v1"><h2>Tackling the Problem of Distributional Shifts: Correcting Misspecified,
  High-Dimensional Data-Driven Priors for Inverse Problems</h2></a>Authors:  Gabriel Missael Barco, Alexandre Adam, Connor Stone, Yashar Hezaveh, Laurence Perreault-Levasseur</br>Comments: 17 pages, 15 figures, Submitted to The Astrophysical Journal</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, cs.LG</br><p>Bayesian inference for inverse problems hinges critically on the choice of
priors. In the absence of specific prior information, population-level
distributions can serve as effective priors for parameters of interest. With
the advent of machine learning, the use of data-driven population-level
distributions (encoded, e.g., in a trained deep neural network) as priors is
emerging as an appealing alternative to simple parametric priors in a variety
of inverse problems. However, in many astrophysical applications, it is often
difficult or even impossible to acquire independent and identically distributed
samples from the underlying data-generating process of interest to train these
models. In these cases, corrupted data or a surrogate, e.g. a simulator, is
often used to produce training samples, meaning that there is a risk of
obtaining misspecified priors. This, in turn, can bias the inferred posteriors
in ways that are difficult to quantify, which limits the potential
applicability of these models in real-world scenarios. In this work, we propose
addressing this issue by iteratively updating the population-level
distributions by retraining the model with posterior samples from different
sets of observations and showcase the potential of this method on the problem
of background image reconstruction in strong gravitational lensing when
score-based models are used as data-driven priors. We show that starting from a
misspecified prior distribution, the updated distribution becomes progressively
closer to the underlying population-level distribution, and the resulting
posterior samples exhibit reduced bias after several updates.</p></br><a href="http://arxiv.org/pdf/2407.15180v1"><h2>Generalizing Trilateration: Approximate Maximum Likelihood Estimator for
  Initial Orbit Determination in Low-Earth Orbit</h2></a>Authors:  Ricardo Ferreira, Filipa Valdeira, Marta Guimarães, Cláudia Soares</br>Comments: No comment found</br>Primary Category: math.OC</br>All Categories: math.OC, astro-ph.IM, cs.LG</br><p>With the increase in the number of active satellites and space debris in
orbit, the problem of initial orbit determination (IOD) becomes increasingly
important, demanding a high accuracy. Over the years, different approaches have
been presented such as filtering methods (for example, Extended Kalman Filter),
differential algebra or solving Lambert's problem. In this work, we consider a
setting of three monostatic radars, where all available measurements are taken
approximately at the same instant. This follows a similar setting as
trilateration, a state-of-the-art approach, where each radar is able to obtain
a single measurement of range and range-rate. Differently, and due to advances
in Multiple-Input Multiple-Output (MIMO) radars, we assume that each location
is able to obtain a larger set of range, angle and Doppler shift measurements.
Thus, our method can be understood as an extension of trilateration leveraging
more recent technology and incorporating additional data. We formulate the
problem as a Maximum Likelihood Estimator (MLE), which for some number of
observations is asymptotically unbiased and asymptotically efficient. Through
numerical experiments, we demonstrate that our method attains the same accuracy
as the trilateration method for the same number of measurements and offers an
alternative and generalization, returning a more accurate estimation of the
satellite's state vector, as the number of available measurements increases.</p></br><a href="http://arxiv.org/pdf/2407.15703v1"><h2>Estimating Probability Densities with Transformer and Denoising
  Diffusion</h2></a>Authors:  Henry W. Leung, Jo Bovy, Joshua S. Speagle</br>Comments: Accepted at the ICML 2024 Workshop on Foundation Models in the Wild</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, stat.ML</br><p>Transformers are often the go-to architecture to build foundation models that
ingest a large amount of training data. But these models do not estimate the
probability density distribution when trained on regression problems, yet
obtaining full probabilistic outputs is crucial to many fields of science,
where the probability distribution of the answer can be non-Gaussian and
multimodal. In this work, we demonstrate that training a probabilistic model
using a denoising diffusion head on top of the Transformer provides reasonable
probability density estimation even for high-dimensional inputs. The combined
Transformer+Denoising Diffusion model allows conditioning the output
probability density on arbitrary combinations of inputs and it is thus a highly
flexible density function emulator of all possible input/output combinations.
We illustrate our Transformer+Denoising Diffusion model by training it on a
large dataset of astronomical observations and measured labels of stars within
our Galaxy and we apply it to a variety of inference tasks to show that the
model can infer labels accurately with reasonable distributions.</p></br>
