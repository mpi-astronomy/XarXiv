search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202410242000+TO+202410302000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, physics.data-an, cs.LG, cs.AI staritng 202410242000 and ending 202410302000</h1>Feed last updated: 2024-10-30T00:00:00-04:00<a href="http://arxiv.org/pdf/2410.20839v1"><h2>Asteroid Mining: ACT&Friends' Results for the GTOC 12 Problem</h2></a>Authors:  Dario Izzo, Marcus Märtens, Laurent Beauregard, Max Bannach, Giacomo Acciarini, Emmanuel Blazquez, Alexander Hadjiivanov, Jai Grover, Gernot Heißel, Yuri Shimane, Chit Hong Yam</br>Comments: No comment found</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, astro-ph.IM, cs.LG</br><p>In 2023, the 12th edition of Global Trajectory Competition was organised
around the problem referred to as "Sustainable Asteroid Mining". This paper
reports the developments that led to the solution proposed by ESA's Advanced
Concepts Team. Beyond the fact that the proposed approach failed to rank higher
than fourth in the final competition leader-board, several innovative
fundamental methodologies were developed which have a broader application. In
particular, new methods based on machine learning as well as on manipulating
the fundamental laws of astrodynamics were developed and able to fill with
remarkable accuracy the gap between full low-thrust trajectories and their
representation as impulsive Lambert transfers. A novel technique was devised to
formulate the challenge of optimal subset selection from a repository of
pre-existing optimal mining trajectories as an integer linear programming
problem. Finally, the fundamental problem of searching for single optimal
mining trajectories (mining and collecting all resources), albeit ignoring the
possibility of having intra-ship collaboration and thus sub-optimal in the case
of the GTOC12 problem, was efficiently solved by means of a novel search based
on a look-ahead score and thus making sure to select asteroids that had chances
to be re-visited later on.</p></br><a href="http://arxiv.org/pdf/2410.21024v1"><h2>Breccia and basalt classification of thin sections of Apollo rocks with
  deep learning</h2></a>Authors:  Freja Thoresen, Aidan Cowley, Romeo Haak, Jonas Lewe, Clara Moriceau, Piotr Knapczyk, Victoria S. Engelschiøn</br>Comments: No comment found</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, astro-ph.IM, cs.LG</br><p>Human exploration of the moon is expected to resume in the next decade,
following the last such activities in the Apollo programme time. One of the
major objectives of returning to the Moon is to continue retrieving geological
samples, with a focus on collecting high-quality specimens to maximize
scientific return. Tools that assist astronauts in making informed decisions
about sample collection activities can maximize the scientific value of future
lunar missions. A lunar rock classifier is a tool that can potentially provide
the necessary information for astronauts to analyze lunar rock samples,
allowing them to augment in-situ value identification of samples. Towards
demonstrating the value of such a tool, in this paper, we introduce a framework
for classifying rock types in thin sections of lunar rocks. We leverage the
vast collection of petrographic thin-section images from the Apollo missions,
captured under plane-polarized light (PPL), cross-polarised light (XPL), and
reflected light at varying magnifications. Advanced machine learning methods,
including contrastive learning, are applied to analyze these images and extract
meaningful features. The contrastive learning approach fine-tunes a pre-trained
Inception-Resnet-v2 network with the SimCLR loss function. The fine-tuned
Inception-Resnet-v2 network can then extract essential features effectively
from the thin-section images of Apollo rocks. A simple binary classifier is
trained using transfer learning from the fine-tuned Inception-ResNet-v2 to
98.44\% ($\pm$1.47) accuracy in separating breccias from basalts.</p></br><a href="http://arxiv.org/pdf/2410.21477v1"><h2>Flow Matching for Atmospheric Retrieval of Exoplanets: Where Reliability
  meets Adaptive Noise Levels</h2></a>Authors:  Timothy D. Gebhard, Jonas Wildberger, Maximilian Dax, Annalena Kofler, Daniel Angerhausen, Sascha P. Quanz, Bernhard Schölkopf</br>Comments: Accepted for publication in Astronomy & Astrophysics</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.EP, cs.LG</br><p>Inferring atmospheric properties of exoplanets from observed spectra is key
to understanding their formation, evolution, and habitability. Since
traditional Bayesian approaches to atmospheric retrieval (e.g., nested
sampling) are computationally expensive, a growing number of machine learning
(ML) methods such as neural posterior estimation (NPE) have been proposed. We
seek to make ML-based atmospheric retrieval (1) more reliable and accurate with
verified results, and (2) more flexible with respect to the underlying neural
networks and the choice of the assumed noise models. First, we adopt flow
matching posterior estimation (FMPE) as a new ML approach to atmospheric
retrieval. FMPE maintains many advantages of NPE, but provides greater
architectural flexibility and scalability. Second, we use importance sampling
(IS) to verify and correct ML results, and to compute an estimate of the
Bayesian evidence. Third, we condition our ML models on the assumed noise level
of a spectrum (i.e., error bars), thus making them adaptable to different noise
models. Both our noise level-conditional FMPE and NPE models perform on par
with nested sampling across a range of noise levels when tested on simulated
data. FMPE trains about 3 times faster than NPE and yields higher IS
efficiencies. IS successfully corrects inaccurate ML results, identifies model
failures via low efficiencies, and provides accurate estimates of the Bayesian
evidence. FMPE is a powerful alternative to NPE for fast, amortized, and
parallelizable atmospheric retrieval. IS can verify results, thus helping to
build confidence in ML-based approaches, while also facilitating model
comparison via the evidence ratio. Noise level conditioning allows design
studies for future instruments to be scaled up, for example, in terms of the
range of signal-to-noise ratios.</p></br><a href="http://arxiv.org/pdf/2410.21076v1"><h2>Accelerated Bayesian parameter estimation and model selection for
  gravitational waves with normalizing flows</h2></a>Authors:  Alicja Polanska, Thibeau Wouters, Peter T. H. Pang, Kaze K. W. Wong, Jason D. McEwen</br>Comments: accepted to NeurIPS 2024 workshop on Machine Learning and the
  Physical Sciences</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.HE, cs.LG, gr-qc</br><p>We present an accelerated pipeline, based on high-performance computing
techniques and normalizing flows, for joint Bayesian parameter estimation and
model selection and demonstrate its efficiency in gravitational wave
astrophysics. We integrate the Jim inference toolkit, a normalizing
flow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned
harmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are
consistent with traditional nested sampling techniques run on $16$ CPU cores,
while reducing the computation time by factors of $5\times$ and $15\times$ for
$4$-dimensional and $11$-dimensional gravitational wave inference problems,
respectively. Our code is available in well-tested and thoroughly documented
open-source packages, ensuring accessibility and reproducibility for the wider
research community.</p></br><a href="http://arxiv.org/pdf/2410.20843v1"><h2>Generative Simulations of The Solar Corona Evolution With Denoising
  Diffusion : Proof of Concept</h2></a>Authors:  Grégoire Francisco, Francesco Pio Ramunno, Manolis K. Georgoulis, João Fernandes, Teresa Barata, Dario Del Moro</br>Comments: No comment found</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, astro-ph.IM, cs.AI</br><p>The solar magnetized corona is responsible for various manifestations with a
space weather impact, such as flares, coronal mass ejections (CMEs) and,
naturally, the solar wind. Modeling the corona's dynamics and evolution is
therefore critical for improving our ability to predict space weather In this
work, we demonstrate that generative deep learning methods, such as Denoising
Diffusion Probabilistic Models (DDPM), can be successfully applied to simulate
future evolutions of the corona as observed in Extreme Ultraviolet (EUV)
wavelengths. Our model takes a 12-hour video of an Active Region (AR) as input
and simulate the potential evolution of the AR over the subsequent 12 hours,
with a time-resolution of two hours. We propose a light UNet backbone
architecture adapted to our problem by adding 1D temporal convolutions after
each classical 2D spatial ones, and spatio-temporal attention in the bottleneck
part. The model not only produce visually realistic outputs but also captures
the inherent stochasticity of the system's evolution. Notably, the simulations
enable the generation of reliable confidence intervals for key predictive
metrics such as the EUV peak flux and fluence of the ARs, paving the way for
probabilistic and interpretable space weather forecasting. Future studies will
focus on shorter forecasting horizons with increased spatial and temporal
resolution, aiming at reducing the uncertainty of the simulations and providing
practical applications for space weather forecasting. The code used for this
study is available at the following link:
https://github.com/gfrancisco20/video_diffusion</p></br><a href="http://arxiv.org/pdf/2410.19390v1"><h2>CLAP. I. Resolving miscalibration for deep learning-based galaxy
  photometric redshift estimation</h2></a>Authors:  Qiufan Lin, Hengxin Ruan, Dominique Fouchez, Shupei Chen, Rui Li, Paulo Montero-Camacho, Nicola R. Napolitano, Yuan-Sen Ting, Wei Zhang</br>Comments: 22 + 6 pages, 9 + 5 figures</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, cs.AI</br><p>Obtaining well-calibrated photometric redshift probability densities for
galaxies without a spectroscopic measurement remains a challenge. Deep learning
discriminative models, typically fed with multi-band galaxy images, can produce
outputs that mimic probability densities and achieve state-of-the-art accuracy.
However, such models may be affected by miscalibration that would result in
discrepancies between the model outputs and the actual distributions of true
redshifts. Our work develops a novel method called the Contrastive Learning and
Adaptive KNN for Photometric Redshift (CLAP) that resolves this issue. It
leverages supervised contrastive learning (SCL) and k-nearest neighbours (KNN)
to construct and calibrate raw probability density estimates, and implements a
refitting procedure to resume end-to-end discriminative models ready to produce
final estimates for large-scale imaging data. The harmonic mean is adopted to
combine an ensemble of estimates from multiple realisations for improving
accuracy. Our experiments demonstrate that CLAP takes advantage of both deep
learning and KNN, outperforming benchmark methods on the calibration of
probability density estimates and retaining high accuracy and computational
efficiency. With reference to CLAP, we point out that miscalibration is
particularly sensitive to the method-induced excessive correlations among data
instances in addition to the unaccounted-for epistemic uncertainties. Reducing
the uncertainties may not guarantee the removal of miscalibration due to the
presence of such excessive correlations, yet this is a problem for conventional
deep learning methods rather than CLAP. These discussions underscore the
robustness of CLAP for obtaining photometric redshift probability densities
required by astrophysical and cosmological applications. This is the first
paper in our series on CLAP.</p></br><a href="http://arxiv.org/pdf/2410.20516v1"><h2>A Cosmic-Scale Benchmark for Symmetry-Preserving Data Processing</h2></a>Authors:  Julia Balla, Siddharth Mishra-Sharma, Carolina Cuesta-Lazaro, Tommi Jaakkola, Tess Smidt</br>Comments: 19 pages, 3 figures; To appear at the NeurReps Workshop @ NeurIPS
  2024</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM</br><p>Efficiently processing structured point cloud data while preserving
multiscale information is a key challenge across domains, from graphics to
atomistic modeling. Using a curated dataset of simulated galaxy positions and
properties, represented as point clouds, we benchmark the ability of graph
neural networks to simultaneously capture local clustering environments and
long-range correlations. Given the homogeneous and isotropic nature of the
Universe, the data exhibits a high degree of symmetry. We therefore focus on
evaluating the performance of Euclidean symmetry-preserving
($E(3)$-equivariant) graph neural networks, showing that they can outperform
non-equivariant counterparts and domain-specific information extraction
techniques in downstream performance as well as simulation-efficiency. However,
we find that current architectures fail to capture information from long-range
correlations as effectively as domain-specific baselines, motivating future
work on architectures better suited for extracting long-range information.</p></br><a href="http://arxiv.org/pdf/2410.19420v1"><h2>Doppler correlation-driven vetoes for the Frequency Hough analysis in
  continuous gravitational-wave searches</h2></a>Authors:  Matteo Di Giovanni, Paola Leaci, Pia Astone, Stefano Dal Pra, Sabrina D'Atonio, Luca D'Onofrio, Sergio Frasca, Federico Muciaccia, Cristiano Palomba, Lorenzo Pierini, Francesco Safai Tehrani</br>Comments: 13 pages, 9 figures, 5 tables</br>Primary Category: gr-qc</br>All Categories: gr-qc, astro-ph.IM, physics.data-an</br><p>We present an improved method for vetoing candidates of continuous
gravitational-wave sources during all-sky searches utilizing the Frequency
Hough pipeline. This approach leverages linear correlations between source
parameters induced by the Earth Doppler effect, which can be effectively
identified through the Hough Transform. Candidates that do not align with these
patterns are considered spurious and can thus be vetoed, enhancing the depth
and statistical significance of follow-up analyses. Additionally, we provide a
comprehensive explanation of the method calibration, which intrinsically linked
to the total duration of the observing run. On average, the procedure
successfully vetoes $56\%$ of candidates. To assess the method performance, we
conducted a Monte-Carlo simulation injecting fake continuous-wave signals into
data from the third observing run of the LIGO detectors. This analysis allowed
us to infer strain amplitude upper limits at a $90\%$ confidence level. We
found that the optimal sensitivity is $h_0^{90\%} = 3.62^{+0.23}_{-0.22}\times
10^{-26}$ in the [128, 200] Hz band, which is within the most sensible
frequency band of the LIGO detectors.</p></br><a href="http://arxiv.org/pdf/2410.20886v1"><h2>CODES: Benchmarking Coupled ODE Surrogates</h2></a>Authors:  Robin Janssen, Immanuel Sulzer, Tobias Buck</br>Comments: 12 pages, 10 figures, accepted for the Machine Learning and the
  Physical Sciences workshop at NeurIPS 2024, source code available on GitHub
  at https://github.com/robin-janssen/CODES-Benchmark</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, physics.comp-ph</br><p>We introduce CODES, a benchmark for comprehensive evaluation of surrogate
architectures for coupled ODE systems. Besides standard metrics like mean
squared error (MSE) and inference time, CODES provides insights into surrogate
behaviour across multiple dimensions like interpolation, extrapolation, sparse
data, uncertainty quantification and gradient correlation. The benchmark
emphasizes usability through features such as integrated parallel training, a
web-based configuration generator, and pre-implemented baseline models and
datasets. Extensive documentation ensures sustainability and provides the
foundation for collaborative improvement. By offering a fair and multi-faceted
comparison, CODES helps researchers select the most suitable surrogate for
their specific dataset and application while deepening our understanding of
surrogate learning behaviour.</p></br><a href="http://arxiv.org/pdf/2410.21374v1"><h2>Model-agnostic basis functions for the 2-point correlation function of
  dark matter in linear theory</h2></a>Authors:  Aseem Paranjape, Ravi K. Sheth</br>Comments: 20 pages, 9 figures, to be submitted to JCAP. The implementation of
  the BiSequential architecture, along with a simple example notebook, is
  publicly available as part of the MLFundas repository at
  https://github.com/a-paranjape/mlfundas</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, cs.LG</br><p>We consider approximating the linearly evolved 2-point correlation function
(2pcf) of dark matter $\xi_{\rm lin}(r;\boldsymbol{\theta})$ in a cosmological
model with parameters $\boldsymbol{\theta}$ as the linear combination $\xi_{\rm
lin}(r;\boldsymbol{\theta})\approx\sum_i\,b_i(r)\,w_i(\boldsymbol{\theta})$,
where the functions $\mathcal{B}=\{b_i(r)\}$ form a $\textit{model-agnostic
basis}$ for the linear 2pcf. This decomposition is important for model-agnostic
analyses of the baryon acoustic oscillation (BAO) feature in the nonlinear 2pcf
of galaxies that fix $\mathcal{B}$ and leave the coefficients $\{w_i\}$ free.
To date, such analyses have made simple but sub-optimal choices for
$\mathcal{B}$, such as monomials. We develop a machine learning framework for
systematically discovering a $\textit{minimal}$ basis $\mathcal{B}$ that
describes $\xi_{\rm lin}(r)$ near the BAO feature in a wide class of
cosmological models. We use a custom architecture, denoted
$\texttt{BiSequential}$, for a neural network (NN) that explicitly realizes the
separation between $r$ and $\boldsymbol{\theta}$ above. The optimal NN trained
on data in which only $\{\Omega_{\rm m},h\}$ are varied in a $\textit{flat}$
$\Lambda$CDM model produces a basis $\mathcal{B}$ comprising $9$ functions
capable of describing $\xi_{\rm lin}(r)$ to $\sim0.6\%$ accuracy in
$\textit{curved}$ $w$CDM models varying 7 parameters within $\sim5\%$ of their
fiducial, flat $\Lambda$CDM values. Scales such as the peak, linear point and
zero-crossing of $\xi_{\rm lin}(r)$ are also recovered with very high accuracy.
We compare our approach to other compression schemes in the literature, and
speculate that $\mathcal{B}$ may also encompass $\xi_{\rm lin}(r)$ in modified
gravity models near our fiducial $\Lambda$CDM model. Using our basis functions
in model-agnostic BAO analyses can potentially lead to significant statistical
gains.</p></br>
