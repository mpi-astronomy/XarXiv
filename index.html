search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202505202000+TO+202505262000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.AI, stat.*, physics.data-an, cs.LG staritng 202505202000 and ending 202505262000</h1>Feed last updated: 2025-05-26T00:00:00-04:00<a href="http://arxiv.org/pdf/2505.16041v1"><h2>Physics-based machine learning for mantle convection simulations</h2></a>Authors:  Siddhant Agarwal, Ali Can Bekar, Christian Hüttig, David S. Greenberg, Nicola Tosi</br>Comments: No comment found</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, cs.LG</br><p>Mantle convection simulations are an essential tool for understanding how
rocky planets evolve. However, the poorly known input parameters to these
simulations, the non-linear dependence of transport properties on pressure and
temperature, and the long integration times in excess of several billion years
all pose a computational challenge for numerical solvers. We propose a
physics-based machine learning approach that predicts creeping flow velocities
as a function of temperature while conserving mass, thereby bypassing the
numerical solution of the Stokes problem. A finite-volume solver then uses the
predicted velocities to advect and diffuse the temperature field to the next
time-step, enabling autoregressive rollout at inference. For training, our
model requires temperature-velocity snapshots from a handful of simulations
(94). We consider mantle convection in a two-dimensional rectangular box with
basal and internal heating, pressure- and temperature-dependent viscosity.
Overall, our model is up to 89 times faster than the numerical solver. We also
show the importance of different components in our convolutional neural network
architecture such as mass conservation, learned paddings on the boundaries, and
loss scaling for the overall rollout performance. Finally, we test our approach
on unseen scenarios to demonstrate some of its strengths and weaknesses.</p></br><a href="http://arxiv.org/pdf/2505.16320v1"><h2>Learning novel representations of variable sources from multi-modal
  $\textit{Gaia}$ data via autoencoders</h2></a>Authors:  P. Huijse, J. De Ridder, L. Eyer, L. Rimoldini, B. Holl, N. Chornay, J. Roquette, K. Nienartowicz, G. Jevardat de Fombelle, D. J. Fritzewski, A. Kemp, V. Vanlaer, M. Vanrespaille, H. Wang, M. I. Carnerero, C. M. Raiteri, G. Marton, M. Madarász, G. Clementini, P. Gavras, C. Aerts</br>Comments: Manuscript resubmitted to Astronomy & Astrophysics after positive
  referee report, 20 pages, 20 figures, 2 tables</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.LG</br><p>Gaia Data Release 3 (DR3) published for the first time epoch photometry,
BP/RP (XP) low-resolution mean spectra, and supervised classification results
for millions of variable sources. This extensive dataset offers a unique
opportunity to study their variability by combining multiple Gaia data
products. In preparation for DR4, we propose and evaluate a machine learning
methodology capable of ingesting multiple Gaia data products to achieve an
unsupervised classification of stellar and quasar variability. A dataset of 4
million Gaia DR3 sources is used to train three variational autoencoders (VAE),
which are artificial neural networks (ANNs) designed for data compression and
generation. One VAE is trained on Gaia XP low-resolution spectra, another on a
novel approach based on the distribution of magnitude differences in the Gaia G
band, and the third on folded Gaia G band light curves. Each Gaia source is
compressed into 15 numbers, representing the coordinates in a 15-dimensional
latent space generated by combining the outputs of these three models. The
learned latent representation produced by the ANN effectively distinguishes
between the main variability classes present in Gaia DR3, as demonstrated
through both supervised and unsupervised classification analysis of the latent
space. The results highlight a strong synergy between light curves and
low-resolution spectral data, emphasising the benefits of combining the
different Gaia data products. A two-dimensional projection of the latent
variables reveals numerous overdensities, most of which strongly correlate with
astrophysical properties, showing the potential of this latent space for
astrophysical discovery. We show that the properties of our novel latent
representation make it highly valuable for variability analysis tasks,
including classification, clustering and outlier detection.</p></br><a href="http://arxiv.org/pdf/2505.17592v1"><h2>AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a
  70B-Parameter Domain-Specialized Reasoning Model</h2></a>Authors:  Tijmen de Haan, Yuan-Sen Ting, Tirthankar Ghosal, Tuan Dung Nguyen, Alberto Accomazzi, Emily Herron, Vanessa Lama, Rui Pan, Azton Wells, Nesar Ramachandra</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.LG</br><p>General-purpose large language models, despite their broad capabilities,
often struggle with specialized domain knowledge, a limitation particularly
pronounced in more accessible, lower-parameter versions. This gap hinders their
deployment as effective agents in demanding fields such as astronomy. Building
on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a
significantly larger and more advanced domain-specialized natural-language AI
assistant. It is designed for research and education across astronomy,
astrophysics, space science, astroparticle physics, cosmology, and astronomical
instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B
underwent extensive continued pre-training on a vast corpus of astronomical
literature, followed by supervised fine-tuning and model merging. Beyond its
70-billion parameter scale, this model incorporates refined datasets,
judiciously chosen learning hyperparameters, and improved training procedures,
achieving state-of-the-art performance on complex astronomical tasks. Notably,
we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to
either answer the user query immediately, or first emit a human-readable
thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425
questions from literature withheld during training -- AstroSage-70B achieves
state-of-the-art performance. It surpasses all other tested open-weight and
proprietary models, including leading systems like o3, Gemini-2.5-Pro,
Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two
orders of magnitude higher. This work demonstrates that domain specialization,
when applied to large-scale models, can enable them to outperform generalist
counterparts in specialized knowledge areas like astronomy, thereby advancing
the frontier of AI capabilities in the field.</p></br><a href="http://arxiv.org/pdf/2505.17814v1"><h2>Searching for extreme mass ratio inspirals in LISA: from identification
  to parameter estimation</h2></a>Authors:  Stefan H. Strub, Lorenzo Speri, Domenico Giardini</br>Comments: No comment found</br>Primary Category: gr-qc</br>All Categories: gr-qc, astro-ph.IM, physics.data-an, physics.space-ph</br><p>The Laser Interferometer Space Antenna (LISA) is a planned space-based
observatory designed to detect gravitational waves (GWs) within the millihertz
frequency range. LISA is anticipated to observe the inspiral of compact objects
into black holes at the centers of galaxies, so called extreme-mass-ratio
inspirals (EMRIs). However, the extraction of these long-lived complex signals
is challenging due to the large size and multimodality of the search space. In
this study, we introduce a new search strategy that allows us to find EMRI
signals in noisy data from wide priors all the way to performing parameter
estimation. This work is an important step in understanding how to extract
EMRIs from future LISA data.</p></br><a href="http://arxiv.org/pdf/2505.14877v1"><h2>A self-regulated convolutional neural network for classifying variable
  stars</h2></a>Authors:  Francisco Pérez-Galarce, Jorge Martínez-Palomera, Karim Pichara, Pablo Huijse, Márcio Catelan</br>Comments: No comment found</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.SR</br><p>Over the last two decades, machine learning models have been widely applied
and have proven effective in classifying variable stars, particularly with the
adoption of deep learning architectures such as convolutional neural networks,
recurrent neural networks, and transformer models. While these models have
achieved high accuracy, they require high-quality, representative data and a
large number of labelled samples for each star type to generalise well, which
can be challenging in time-domain surveys. This challenge often leads to models
learning and reinforcing biases inherent in the training data, an issue that is
not easily detectable when validation is performed on subsamples from the same
catalogue. The problem of biases in variable star data has been largely
overlooked, and a definitive solution has yet to be established. In this paper,
we propose a new approach to improve the reliability of classifiers in variable
star classification by introducing a self-regulated training process. This
process utilises synthetic samples generated by a physics-enhanced latent space
variational autoencoder, incorporating six physical parameters from Gaia Data
Release 3. Our method features a dynamic interaction between a classifier and a
generative model, where the generative model produces ad-hoc synthetic light
curves to reduce confusion during classifier training and populate
underrepresented regions in the physical parameter space. Experiments conducted
under various scenarios demonstrate that our self-regulated training approach
outperforms traditional training methods for classifying variable stars on
biased datasets, showing statistically significant improvements.</p></br>
