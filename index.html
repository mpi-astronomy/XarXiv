search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202510112000+TO+202510172000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, cs.AI, stat.*, physics.data-an staritng 202510112000 and ending 202510172000</h1>Feed last updated: 2025-10-17T00:00:00-04:00<a href="http://arxiv.org/pdf/2510.10713v1"><h2>Deep Learning in Astrophysics</h2></a>Authors:  Yuan-Sen Ting</br>Comments: Manuscript submitted to Annual Review of Astronomy and Astrophysics
  for Volume 64. This is the authors' version. Revisions and the final version
  will be available at https://www.annualreviews.org/content/journals/astro</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, astro-ph.EP, astro-ph.GA, astro-ph.HE, cs.AI</br><p>Deep learning has generated diverse perspectives in astronomy, with ongoing
discussions between proponents and skeptics motivating this review. We examine
how neural networks complement classical statistics, extending our data
analytical toolkit for modern surveys. Astronomy offers unique opportunities
through encoding physical symmetries, conservation laws, and differential
equations directly into architectures, creating models that generalize beyond
training data. Yet challenges persist as unlabeled observations number in
billions while confirmed examples with known properties remain scarce and
expensive. This review demonstrates how deep learning incorporates domain
knowledge through architectural design, with built-in assumptions guiding
models toward physically meaningful solutions. We evaluate where these methods
offer genuine advances versus claims requiring careful scrutiny. - Neural
architectures overcome trade-offs between scalability, expressivity, and data
efficiency by encoding physical symmetries and conservation laws into network
structure, enabling learning from limited labeled data. - Simulation-based
inference and anomaly detection extract information from complex, non-Gaussian
distributions where analytical likelihoods fail, enabling field-level
cosmological analysis and systematic discovery of rare phenomena. - Multi-scale
neural modeling bridges resolution gaps in astronomical simulations, learning
effective subgrid physics from expensive high-fidelity runs to enhance
large-volume calculations where direct computation remains prohibitive. -
Emerging paradigms-reinforcement learning for telescope operations, foundation
models learning from minimal examples, and large language model agents for
research automation-show promise though are still developing in astronomical
applications.</p></br><a href="http://arxiv.org/pdf/2510.12958v1"><h2>Simulation-Based Pretraining and Domain Adaptation for Astronomical Time
  Series with Minimal Labeled Data</h2></a>Authors:  Rithwik Gupta, Daniel Muthukrishna, Jeroen Audenaert</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.HE, astro-ph.SR, cs.LG</br><p>Astronomical time-series analysis faces a critical limitation: the scarcity
of labeled observational data. We present a pre-training approach that
leverages simulations, significantly reducing the need for labeled examples
from real observations. Our models, trained on simulated data from multiple
astronomical surveys (ZTF and LSST), learn generalizable representations that
transfer effectively to downstream tasks. Using classifier-based architectures
enhanced with contrastive and adversarial objectives, we create domain-agnostic
models that demonstrate substantial performance improvements over baseline
methods in classification, redshift estimation, and anomaly detection when
fine-tuned with minimal real data. Remarkably, our models exhibit effective
zero-shot transfer capabilities, achieving comparable performance on future
telescope (LSST) simulations when trained solely on existing telescope (ZTF)
data. Furthermore, they generalize to very different astronomical phenomena
(namely variable stars from NASA's \textit{Kepler} telescope) despite being
trained on transient events, demonstrating cross-domain capabilities. Our
approach provides a practical solution for building general models when labeled
data is scarce, but domain knowledge can be encoded in simulations.</p></br><a href="http://arxiv.org/pdf/2510.11242v1"><h2>Analyzing Data Quality and Decay in Mega-Constellations: A
  Physics-Informed Machine Learning Approach</h2></a>Authors:  Katarina Dyreby, Francisco Caldas, Cl√°udia Soares</br>Comments: 76th International Astronautical Congress</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, astro-ph.IM, cs.LG</br><p>In the era of mega-constellations, the need for accurate and publicly
available information has become fundamental for satellite operators to
guarantee the safety of spacecrafts and the Low Earth Orbit (LEO) space
environment. This study critically evaluates the accuracy and reliability of
publicly available ephemeris data for a LEO mega-constellation - Starlink. The
goal of this work is twofold: (i) compare and analyze the quality of the data
against high-precision numerical propagation. (ii) Leverage Physics-Informed
Machine Learning to extract relevant satellite quantities, such as
non-conservative forces, during the decay process. By analyzing two months of
real orbital data for approximately 1500 Starlink satellites, we identify
discrepancies between high precision numerical algorithms and the published
ephemerides, recognizing the use of simplified dynamics at fixed thresholds,
planned maneuvers, and limitations in uncertainty propagations. Furthermore, we
compare data obtained from multiple sources to track and analyze deorbiting
satellites over the same period. Empirically, we extract the acceleration
profile of satellites during deorbiting and provide insights relating to the
effects of non-conservative forces during reentry. For non-deorbiting
satellites, the position Root Mean Square Error (RMSE) was approximately 300 m,
while for deorbiting satellites it increased to about 600 m. Through this
in-depth analysis, we highlight potential limitations in publicly available
data for accurate and robust Space Situational Awareness (SSA), and
importantly, we propose a data-driven model of satellite decay in
mega-constellations.</p></br><a href="http://arxiv.org/pdf/2510.13968v1"><h2>Constraining Power of Wavelet vs. Power Spectrum Statistics for CMB
  Lensing and Weak Lensing with Learned Binning</h2></a>Authors:  Kyle Boone, Georgios Valogiannis, Marco Gatti, Cora Dvorkin</br>Comments: 21 pages, 15 figures, 4 tables</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, astro-ph.IM, physics.data-an</br><p>We present forecasts for constraints on the matter density ($\Omega_m$) and
the amplitude of matter density fluctuations at 8h$^{-1}$Mpc ($\sigma_8$) from
CMB lensing convergence maps and galaxy weak lensing convergence maps. For CMB
lensing convergence auto statistics, we compare the angular power spectra
($C_\ell$'s) to the wavelet scattering transform (WST) coefficients. For CMB
lensing convergence $\times$ galaxy weak lensing convergence statistics, we
compare the cross angular power spectra to wavelet phase harmonics (WPH). This
work also serves as the first application of WST and WPH to these probes. For
CMB lensing convergence, we find that WST and $C_\ell$'s yield similar
constraints in forecasts for the $\textit{Simons}$ Observatory and the South
Pole Telescope. However, WST gives a tighter constraint on $\sigma_8$ by a
factor of $1.7$ for $\textit{Planck}$ data. When CMB lensing convergence is
crossed with galaxy weak lensing convergence projected from $\textit{Euclid}$
Data Release 2 (DR2), we find that WPH outperforms cross-$C_\ell$'s by factors
between $2.4$ and $3.8$ for individual parameter constraints. To compare these
different summary statistics we develop a novel learned binning approach. This
method compresses summary statistics while maintaining interpretability. We
find this leads to improved constraints compared to more naive binning schemes
for $C_\ell$'s, WST, and most significantly WPH. By learning the binning and
measuring constraints on distinct data sets, our method is robust to
overfitting by construction.</p></br><a href="http://arxiv.org/pdf/2510.13997v1"><h2>Dynamic SBI: Round-free Sequential Simulation-Based Inference with
  Adaptive Datasets</h2></a>Authors:  Huifang Lyu, James Alvey, Noemi Anau Montel, Mauro Pieroni, Christoph Weniger</br>Comments: 15 pages, 5 figures, software available at:
  https://github.com/dynamic-sbi</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, cs.LG, stat.ML</br><p>Simulation-based inference (SBI) is emerging as a new statistical paradigm
for addressing complex scientific inference problems. By leveraging the
representational power of deep neural networks, SBI can extract the most
informative simulation features for the parameters of interest. Sequential SBI
methods extend this approach by iteratively steering the simulation process
towards the most relevant regions of parameter space. This is typically
implemented through an algorithmic structure, in which simulation and network
training alternate over multiple rounds. This strategy is particularly well
suited for high-precision inference in high-dimensional settings, which are
commonplace in physics applications with growing data volumes and increasing
model fidelity. Here, we introduce dynamic SBI, which implements the core ideas
of sequential methods in a round-free, asynchronous, and highly parallelisable
manner. At its core is an adaptive dataset that is iteratively transformed
during inference to resemble the target observation. Simulation and training
proceed in parallel: trained networks are used both to filter out simulations
incompatible with the data and to propose new, more promising ones. Compared to
round-based sequential methods, this asynchronous structure can significantly
reduce simulation costs and training overhead. We demonstrate that dynamic SBI
achieves significant improvements in simulation and training efficiency while
maintaining inference performance. We further validate our framework on two
challenging astrophysical inference tasks: characterising the stochastic
gravitational wave background and analysing strong gravitational lensing
systems. Overall, this work presents a flexible and efficient new paradigm for
sequential SBI.</p></br><a href="http://arxiv.org/pdf/2510.12920v1"><h2>InferA: A Smart Assistant for Cosmological Ensemble Data</h2></a>Authors:  Justin Z. Tam, Pascal Grosset, Divya Banesh, Nesar Ramachandra, Terece L. Turton, James Ahrens</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI</br><p>Analyzing large-scale scientific datasets presents substantial challenges due
to their sheer volume, structural complexity, and the need for specialized
domain knowledge. Automation tools, such as PandasAI, typically require full
data ingestion and lack context of the full data structure, making them
impractical as intelligent data analysis assistants for datasets at the
terabyte scale. To overcome these limitations, we propose InferA, a multi-agent
system that leverages large language models to enable scalable and efficient
scientific data analysis. At the core of the architecture is a supervisor agent
that orchestrates a team of specialized agents responsible for distinct phases
of the data retrieval and analysis. The system engages interactively with users
to elicit their analytical intent and confirm query objectives, ensuring
alignment between user goals and system actions. To demonstrate the framework's
usability, we evaluate the system using ensemble runs from the HACC cosmology
simulation which comprises several terabytes.</p></br><a href="http://arxiv.org/pdf/2510.14102v1"><h2>Extracting latent representations from X-ray spectra. Classification,
  regression, and accretion signatures of Chandra sources</h2></a>Authors:  Nicol√≤ Oreste Pinciroli Vago, Juan Rafael Mart√≠nez-Galarza, Roberta Amato</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI, cs.LG</br><p>The study of X-ray spectra is crucial to understanding the physical nature of
astrophysical sources. Machine learning methods can extract compact and
informative representations of data from large datasets. The Chandra Source
Catalog (CSC) provides a rich archive of X-ray spectral data, which remains
largely underexplored in this context. This work aims to develop a compact and
physically meaningful representation of Chandra X-ray spectra using deep
learning. To verify that the learned representation captures relevant
information, we evaluate it through classification, regression, and
interpretability analyses. We use a transformer-based autoencoder to compress
X-ray spectra. The input spectra, drawn from the CSC, include only
high-significance detections. Astrophysical source types and physical summary
statistics are compiled from external catalogs. We evaluate the learned
representation in terms of spectral reconstruction accuracy, clustering
performance on 8 known astrophysical source classes, and correlation with
physical quantities such as hardness ratios and hydrogen column density
($N_H$). The autoencoder accurately reconstructs spectra with 8 latent
variables. Clustering in the latent space yields a balanced classification
accuracy of $\sim$40% across the 8 source classes, increasing to $\sim$69% when
restricted to AGNs and stellar-mass compact objects exclusively. Moreover,
latent features correlate with non-linear combinations of spectral fluxes,
suggesting that the compressed representation encodes physically relevant
information. The proposed autoencoder-based pipeline is a powerful tool for the
representation and interpretation of X-ray spectra, providing a compact latent
space that supports both classification and the estimation of physical
properties. This work demonstrates the potential of deep learning for spectral
studies and uncovering new patterns in X-ray data.</p></br>
