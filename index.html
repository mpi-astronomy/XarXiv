search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202504092000+TO+202504152000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.AI, cs.LG, stat.*, physics.data-an staritng 202504092000 and ending 202504152000</h1>Feed last updated: 2025-04-15T00:00:00-04:00<a href="http://arxiv.org/pdf/2504.07777v1"><h2>Adaptive Detection of Fast Moving Celestial Objects Using a Mixture of
  Experts and Physical-Inspired Neural Network</h2></a>Authors:  Peng Jia, Ge Li, Bafeng Cheng, Yushan Li, Rongyu Sun</br>Comments: Accepted by the AJ</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.EP, cs.CV, cs.LG, physics.optics</br><p>Fast moving celestial objects are characterized by velocities across the
celestial sphere that significantly differ from the motions of background
stars. In observational images, these objects exhibit distinct shapes,
contrasting with the typical appearances of stars. Depending on the
observational method employed, these celestial entities may be designated as
near-Earth objects or asteroids. Historically, fast moving celestial objects
have been observed using ground-based telescopes, where the relative stability
of stars and Earth facilitated effective image differencing techniques
alongside traditional fast moving celestial object detection and classification
algorithms. However, the growing prevalence of space-based telescopes, along
with their diverse observational modes, produces images with different
properties, rendering conventional methods less effective. This paper presents
a novel algorithm for detecting fast moving celestial objects within star
fields. Our approach enhances state-of-the-art fast moving celestial object
detection neural networks by transforming them into physical-inspired neural
networks. These neural networks leverage the point spread function of the
telescope and the specific observational mode as prior information; they can
directly identify moving fast moving celestial objects within star fields
without requiring additional training, thereby addressing the limitations of
traditional techniques. Additionally, all neural networks are integrated using
the mixture of experts technique, forming a comprehensive fast moving celestial
object detection algorithm. We have evaluated our algorithm using simulated
observational data that mimics various observations carried out by space based
telescope scenarios and real observation images. Results demonstrate that our
method effectively detects fast moving celestial objects across different
observational modes.</p></br><a href="http://arxiv.org/pdf/2504.08583v1"><h2>AstroLLaVA: towards the unification of astronomical data and natural
  language</h2></a>Authors:  Sharaf Zaman, Michael J. Smith, Pranav Khetarpal, Rishabh Chakrabarty, Michele Ginolfi, Marc Huertas-Company, Maja Jabłońska, Sandor Kruk, Matthieu Le Lain, Sergio José Rodríguez Méndez, Dimitrios Tanoglidis</br>Comments: 8 pages, 3 figures, accepted to SCI-FM@ICLR 2025. Code at
  https://w3id.org/UniverseTBD/AstroLLaVA</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.LG</br><p>We present AstroLLaVA, a vision language model for astronomy that enables
interaction with astronomical imagery through natural dialogue. By fine-tuning
the LLaVA model on a diverse dataset of $\sim$30k images with captions and
question-answer pairs sourced from NASA's `Astronomy Picture of the Day', the
European Southern Observatory, and the NASA/ESA Hubble Space Telescope, we
create a model capable of answering open-ended questions about astronomical
concepts depicted visually. Our two-stage fine-tuning process adapts the model
to both image captioning and visual question answering in the astronomy domain.
We demonstrate AstroLLaVA's performance on an astronomical visual question
answering benchmark and release the model weights, code, and training set to
encourage further open source work in this space. Finally, we suggest a roadmap
towards general astronomical data alignment with pre-trained language models,
and provide an open space for collaboration towards this end for interested
researchers.</p></br>
