search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202510032000+TO+202510092000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, stat.*, cs.AI, physics.data-an staritng 202510032000 and ending 202510092000</h1>Feed last updated: 2025-10-09T00:00:00-04:00<a href="http://arxiv.org/pdf/2510.06200v1"><h2>StarEmbed: Benchmarking Time Series Foundation Models on Astronomical
  Observations of Variable Stars</h2></a>Authors:  Weijian Li, Hong-Yu Chen, Qinjie Lin, Nabeel Rehemtulla, Ved G. Shah, Dennis Wu, Adam A. Miller, Han Liu</br>Comments: No comment found</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, astro-ph.IM, cs.AI</br><p>Time series foundation models (TSFMs) are increasingly being adopted as
highly-capable general-purpose time series representation learners. Although
their training corpora are vast, they exclude astronomical time series data.
Observations of stars produce peta-scale time series with unique challenges
including irregular sampling and heteroskedasticity. We introduce StarEmbed,
the first public benchmark for rigorous and standardized evaluation of
state-of-the-art TSFMs on stellar time series observations (``light curves'').
We benchmark on three scientifically-motivated downstream tasks: unsupervised
clustering, supervised classification, and out-of-distribution source
detection. StarEmbed integrates a catalog of expert-vetted labels with
multi-variate light curves from the Zwicky Transient Facility, yielding ~40k
hand-labeled light curves spread across seven astrophysical classes. We
evaluate the zero-shot representation capabilities of three TSFMs (MOIRAI,
Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against
handcrafted feature extraction, the long-standing baseline in the astrophysics
literature. Our results demonstrate that these TSFMs, especially the Chronos
models, which are trained on data completely unlike the astronomical
observations, can outperform established astrophysics-specific baselines in
some tasks and effectively generalize to entirely new data. In particular,
TSFMs deliver state-of-the-art performance on our out-of-distribution source
detection benchmark. With the first benchmark of TSFMs on astronomical time
series data, we test the limits of their generalization and motivate a paradigm
shift in time-domain astronomy from using task-specific, fully supervised
pipelines toward adopting generic foundation model representations for the
analysis of peta-scale datasets from forthcoming observatories.</p></br><a href="http://arxiv.org/pdf/2510.06931v1"><h2>Textual interpretation of transient image classifications from large
  language models</h2></a>Authors:  Fiorenzo Stoppa, Turan Bulmus, Steven Bloemen, Stephen J. Smartt, Paul J. Groot, Paul Vreeswijk, Ken W. Smith</br>Comments: Published in Nature Astronomy (2025). Publisher's Version of Record
  (CC BY 4.0). DOI: 10.1038/s41550-025-02670-z</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.LG</br><p>Modern astronomical surveys deliver immense volumes of transient detections,
yet distinguishing real astrophysical signals (for example, explosive events)
from bogus imaging artefacts remains a challenge. Convolutional neural networks
are effectively used for real versus bogus classification; however, their
reliance on opaque latent representations hinders interpretability. Here we
show that large language models (LLMs) can approach the performance level of a
convolutional neural network on three optical transient survey datasets
(Pan-STARRS, MeerLICHT and ATLAS) while simultaneously producing direct,
human-readable descriptions for every candidate. Using only 15 examples and
concise instructions, Google's LLM, Gemini, achieves a 93% average accuracy
across datasets that span a range of resolution and pixel scales. We also show
that a second LLM can assess the coherence of the output of the first model,
enabling iterative refinement by identifying problematic cases. This framework
allows users to define the desired classification behaviour through natural
language and examples, bypassing traditional training pipelines. Furthermore,
by generating textual descriptions of observed features, LLMs enable users to
query classifications as if navigating an annotated catalogue, rather than
deciphering abstract latent spaces. As next-generation telescopes and surveys
further increase the amount of data available, LLM-based classification could
help bridge the gap between automated detection and transparent, human-level
understanding.</p></br><a href="http://arxiv.org/pdf/2510.04762v1"><h2>Fisher-Bingham-like normalizing flows on the sphere</h2></a>Authors:  Thorsten Gl√ºsenkamp</br>Comments: No comment found</br>Primary Category: stat.ML</br>All Categories: stat.ML, astro-ph.IM, cs.AI, cs.LG</br><p>A generic D-dimensional Gaussian can be conditioned or projected onto the D-1
unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular
Gaussian (AG) distribution families, respectively. These are some of the most
fundamental distributions on the sphere, yet cannot straightforwardly be
written as a normalizing flow except in two special cases: the von-Mises Fisher
in D=3 and the central angular Gaussian in any D. In this paper, we describe
how to generalize these special cases to a family of normalizing flows that
behave similarly to the full FB or AG family in any D. We call them
"zoom-linear-project" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham
distribution, their composition allows to gradually add complexity as needed.
Furthermore, they can naturally handle conditional density estimation with
target distributions that vary by orders of magnitude in scale - a setting that
is important in astronomical applications but that existing flows often
struggle with. A particularly useful member of the new family is the Kent
analogue that can cheaply upgrade any flow in this situation to yield better
performance.</p></br><a href="http://arxiv.org/pdf/2510.06273v1"><h2>Vision Transformer for Transient Noise Classification</h2></a>Authors:  Divyansh Srivastava, Andrzej Niedzielski</br>Comments: 9 pages, 4 figures</br>Primary Category: cs.CV</br>All Categories: cs.CV, astro-ph.IM, cs.LG, gr-qc</br><p>Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning</p></br><a href="http://arxiv.org/pdf/2510.05205v1"><h2>A Data-Driven Prism: Multi-View Source Separation with Diffusion Model
  Priors</h2></a>Authors:  Sebastian Wagner-Carena, Aizhan Akhmetzhanova, Sydney Erickson</br>Comments: Accepted to main conference of NeurIPS 2025. Code available at
  https://github.com/swagnercarena/ddprism</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.CO</br><p>A common challenge in the natural sciences is to disentangle distinct,
unknown sources from observations. Examples of this source separation task
include deblending galaxies in a crowded field, distinguishing the activity of
individual neurons from overlapping signals, and separating seismic events from
an ambient background. Traditional analyses often rely on simplified source
models that fail to accurately reproduce the data. Recent advances have shown
that diffusion models can directly learn complex prior distributions from
noisy, incomplete data. In this work, we show that diffusion models can solve
the source separation problem without explicit assumptions about the source.
Our method relies only on multiple views, or the property that different sets
of observations contain different linear transformations of the unknown
sources. We show that our method succeeds even when no source is individually
observed and the observations are noisy, incomplete, and vary in resolution.
The learned diffusion models enable us to sample from the source priors,
evaluate the probability of candidate sources, and draw from the joint
posterior of the source distribution given an observation. We demonstrate the
effectiveness of our method on a range of synthetic problems as well as
real-world galaxy observations.</p></br><a href="http://arxiv.org/pdf/2510.05399v1"><h2>Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for
  24-Hour Solar Proton Flux Profiles Using GOES Data</h2></a>Authors:  Kangwoo Yi, Bo Shen, Qin Li, Haimin Wang, Yong-Jae Moon, Jaewon Lee, Hwanhee Lee</br>Comments: 7 pages; accepted as a workshop paper at ICDM 2025</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.SR, cs.AI</br><p>Solar Proton Events (SPEs) cause significant radiation hazards to satellites,
astronauts, and technological systems. Accurate forecasting of their proton
flux time profiles is crucial for early warnings and mitigation. This paper
explores deep learning sequence-to-sequence (seq2seq) models based on Long
Short-Term Memory networks to predict 24-hour proton flux profiles following
SPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by
NOAA GOES, each associated with a >=M-class western-hemisphere solar flare and
undisturbed proton flux profiles. Using 4-fold stratified cross-validation, we
evaluate seq2seq model configurations (varying hidden units and embedding
dimensions) under multiple forecasting scenarios: (i) proton-only input vs.
combined proton+X-ray input, (ii) original flux data vs. trend-smoothed data,
and (iii) autoregressive vs. one-shot forecasting. Our major results are as
follows: First, one-shot forecasting consistently yields lower error than
autoregressive prediction, avoiding the error accumulation seen in iterative
approaches. Second, on the original data, proton-only models outperform
proton+X-ray models. However, with trend-smoothed data, this gap narrows or
reverses in proton+X-ray models. Third, trend-smoothing significantly enhances
the performance of proton+X-ray models by mitigating fluctuations in the X-ray
channel. Fourth, while models trained on trendsmoothed data perform best on
average, the best-performing model was trained on original data, suggesting
that architectural choices can sometimes outweigh the benefits of data
preprocessing.</p></br>
