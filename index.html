search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202407172000+TO+202407232000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.AI, cs.LG, physics.data-an, stat.* staritng 202407172000 and ending 202407232000</h1>Feed last updated: 2024-07-23T00:00:00-04:00<a href="http://arxiv.org/pdf/2407.15703v1"><h2>Estimating Probability Densities with Transformer and Denoising
  Diffusion</h2></a>Authors:  Henry W. Leung, Jo Bovy, Joshua S. Speagle</br>Comments: Accepted at the ICML 2024 Workshop on Foundation Models in the Wild</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, stat.ML</br><p>Transformers are often the go-to architecture to build foundation models that
ingest a large amount of training data. But these models do not estimate the
probability density distribution when trained on regression problems, yet
obtaining full probabilistic outputs is crucial to many fields of science,
where the probability distribution of the answer can be non-Gaussian and
multimodal. In this work, we demonstrate that training a probabilistic model
using a denoising diffusion head on top of the Transformer provides reasonable
probability density estimation even for high-dimensional inputs. The combined
Transformer+Denoising Diffusion model allows conditioning the output
probability density on arbitrary combinations of inputs and it is thus a highly
flexible density function emulator of all possible input/output combinations.
We illustrate our Transformer+Denoising Diffusion model by training it on a
large dataset of astronomical observations and measured labels of stars within
our Galaxy and we apply it to a variety of inference tasks to show that the
model can infer labels accurately with reasonable distributions.</p></br>
