search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202407232000+TO+202407292000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, stat.*, physics.data-an, cs.AI staritng 202407232000 and ending 202407292000</h1>Feed last updated: 2024-07-29T00:00:00-04:00<a href="http://arxiv.org/pdf/2407.16917v1"><h2>TelescopeML -- I. An End-to-End Python Package for Interpreting
  Telescope Datasets through Training Machine Learning Models, Generating
  Statistical Reports, and Visualizing Results</h2></a>Authors:  Ehsan, Gharib-Nezhad, Natasha E. Batalha, Hamed Valizadegan, Miguel J. S. Martinho, Mahdi Habibi, Gopal Nookula</br>Comments: Please find the accepted paper with complete reference list at
  https://joss.theoj.org/papers/10.21105/joss.06346</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.EP, cs.LG</br><p>We are on the verge of a revolutionary era in space exploration, thanks to
advancements in telescopes such as the James Webb Space Telescope
(\textit{JWST}). High-resolution, high signal-to-noise spectra from exoplanet
and brown dwarf atmospheres have been collected over the past few decades,
requiring the development of accurate and reliable pipelines and tools for
their analysis. Accurately and swiftly determining the spectroscopic parameters
from the observational spectra of these objects is crucial for understanding
their atmospheric composition and guiding future follow-up observations.
\texttt{TelescopeML} is a Python package developed to perform three main tasks:
1. Process the synthetic astronomical datasets for training a CNN model and
prepare the observational dataset for later use for prediction; 2. Train a CNN
model by implementing the optimal hyperparameters; and 3. Deploy the trained
CNN models on the actual observational data to derive the output spectroscopic
parameters.</p></br><a href="http://arxiv.org/pdf/2407.17667v1"><h2>Tackling the Problem of Distributional Shifts: Correcting Misspecified,
  High-Dimensional Data-Driven Priors for Inverse Problems</h2></a>Authors:  Gabriel Missael Barco, Alexandre Adam, Connor Stone, Yashar Hezaveh, Laurence Perreault-Levasseur</br>Comments: 17 pages, 15 figures, Submitted to The Astrophysical Journal</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, cs.LG</br><p>Bayesian inference for inverse problems hinges critically on the choice of
priors. In the absence of specific prior information, population-level
distributions can serve as effective priors for parameters of interest. With
the advent of machine learning, the use of data-driven population-level
distributions (encoded, e.g., in a trained deep neural network) as priors is
emerging as an appealing alternative to simple parametric priors in a variety
of inverse problems. However, in many astrophysical applications, it is often
difficult or even impossible to acquire independent and identically distributed
samples from the underlying data-generating process of interest to train these
models. In these cases, corrupted data or a surrogate, e.g. a simulator, is
often used to produce training samples, meaning that there is a risk of
obtaining misspecified priors. This, in turn, can bias the inferred posteriors
in ways that are difficult to quantify, which limits the potential
applicability of these models in real-world scenarios. In this work, we propose
addressing this issue by iteratively updating the population-level
distributions by retraining the model with posterior samples from different
sets of observations and showcase the potential of this method on the problem
of background image reconstruction in strong gravitational lensing when
score-based models are used as data-driven priors. We show that starting from a
misspecified prior distribution, the updated distribution becomes progressively
closer to the underlying population-level distribution, and the resulting
posterior samples exhibit reduced bias after several updates.</p></br><a href="http://arxiv.org/pdf/2407.18647v1"><h2>Towards unveiling the large-scale nature of gravity with the wavelet
  scattering transform</h2></a>Authors:  Georgios Valogiannis, Francisco Villaescusa-Navarro, Marco Baldi</br>Comments: 19 pages, 15 figures, 1 table</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, gr-qc, hep-ph, physics.data-an</br><p>We present the first application of the Wavelet Scattering Transform (WST) in
order to constrain the nature of gravity using the three-dimensional (3D)
large-scale structure of the universe. Utilizing the Quijote-MG N-body
simulations, we can reliably model the 3D matter overdensity field for the f(R)
Hu-Sawicki modified gravity (MG) model down to $k_{\rm max}=0.5$ h/Mpc.
Combining these simulations with the Quijote $\nu$CDM collection, we then
conduct a Fisher forecast of the marginalized constraints obtained on gravity
using the WST coefficients and the matter power spectrum at redshift z=0. Our
results demonstrate that the WST substantially improves upon the 1$\sigma$
error obtained on the parameter that captures deviations from standard General
Relativity (GR), yielding a tenfold improvement compared to the corresponding
matter power spectrum result. At the same time, the WST also enhances the
precision on the $\Lambda$CDM parameters and the sum of neutrino masses, by
factors of 1.2-3.4 compared to the matter power spectrum, respectively. Despite
the overall reduction in the WST performance when we focus on larger scales, it
still provides a relatively $4.5\times$ tighter 1$\sigma$ error for the MG
parameter at $k_{\rm max}=0.2$ h/Mpc, highlighting its great sensitivity to the
underlying gravity theory. This first proof-of-concept study reaffirms the
constraining properties of the WST technique and paves the way for exciting
future applications in order to perform precise large-scale tests of gravity
with the new generation of cutting-edge cosmological data.</p></br><a href="http://arxiv.org/pdf/2407.18909v1"><h2>Hybrid summary statistics: neural weak lensing inference beyond the
  power spectrum</h2></a>Authors:  T. Lucas Makinen, Tom Charnock, Natalia Porqueres, Axel Lapel, Alan Heavens, Benjamin D. Wandelt</br>Comments: 16 pages, 11 figures. Submitted to JCAP. We provide publicly
  available code at https://github.com/tlmakinen/hybridStatsWL</br>Primary Category: astro-ph.CO</br>All Categories: astro-ph.CO, cs.LG, physics.comp-ph, stat.ML, stat.OT</br><p>In inference problems, we often have domain knowledge which allows us to
define summary statistics that capture most of the information content in a
dataset. In this paper, we present a hybrid approach, where such physics-based
summaries are augmented by a set of compressed neural summary statistics that
are optimised to extract the extra information that is not captured by the
predefined summaries. The resulting statistics are very powerful inputs to
simulation-based or implicit inference of model parameters. We apply this
generalisation of Information Maximising Neural Networks (IMNNs) to parameter
constraints from tomographic weak gravitational lensing convergence maps to
find summary statistics that are explicitly optimised to complement angular
power spectrum estimates. We study several dark matter simulation resolutions
in low- and high-noise regimes. We show that i) the information-update
formalism extracts at least $3\times$ and up to $8\times$ as much information
as the angular power spectrum in all noise regimes, ii) the network summaries
are highly complementary to existing 2-point summaries, and iii) our formalism
allows for networks with smaller, physically-informed architectures to match
much larger regression networks with far fewer simulations needed to obtain
asymptotically optimal inference.</p></br>
