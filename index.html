search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202507052000+TO+202507112000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, cs.LG, physics.data-an, cs.AI staritng 202507052000 and ending 202507112000</h1>Feed last updated: 2025-07-11T00:00:00-04:00<a href="http://arxiv.org/pdf/2507.06217v1"><h2>What ZTF Saw Where Rubin Looked: Anomaly Hunting in DR23</h2></a>Authors:  Maria V. Pruzhinskaya, Anastasia D. Lavrukhina, Timofey A. Semenikhi, Alina A. Volnova, Sreevarsha Sreejith, Vadim V. Krushinsky, Emmanuel Gangler, Emille E. O. Ishida, Matwey V. Kornilov, Konstantin L. Malanchev</br>Comments: 11 pages, 4 figures</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.GA, astro-ph.SR, cs.LG</br><p>We present results from the SNAD VIII Workshop, during which we conducted the
first systematic anomaly search in the ZTF fields also observed by LSSTComCam
during Rubin Scientific Pipeline commissioning. Using the PineForest active
anomaly detection algorithm, we analysed four selected fields (two galactic and
two extragalactic) and visually inspected 400 candidates. As a result, we
discovered six previously uncatalogued variable stars, including RS~CVn, BY
Draconis, ellipsoidal, and solar-type variables, and refined classifications
and periods for six known objects. These results demonstrate the effectiveness
of the SNAD anomaly detection pipeline and provide a preview of the discovery
potential in the upcoming LSST data.</p></br><a href="http://arxiv.org/pdf/2507.05333v1"><h2>Causal Foundation Models: Disentangling Physics from Instrument
  Properties</h2></a>Authors:  Jeroen Audenaert, Daniel Muthukrishna, Paul F. Gregory, David W. Hogg, V. Ashley Villar</br>Comments: 8 pages, 5 figures. Accepted to the ICML 2025 Foundation Models for
  Structured Data Workshop and accepted to the Machine Learning for
  Astrophysics Workshop 2025</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, astro-ph.SR, cs.AI</br><p>Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.</p></br><a href="http://arxiv.org/pdf/2507.07155v1"><h2>Evaluating Retrieval-Augmented Generation Agents for Autonomous
  Scientific Discovery in Astrophysics</h2></a>Authors:  Xueqing Xu, Boris Bolliet, Adrian Dimitrov, Andrew Laverick, Francisco Villaescusa-Navarro, Licong Xu, Íñigo Zubeldia</br>Comments: Accepted contribution (spotlight) to the ICML 2025 Workshop on
  Machine Learning for Astrophysics; codes:
  https://huggingface.co/datasets/ASTROANTS/CosmoPaperQA,
  https://github.com/CMBAgents/cmbagent, https://github.com/CMBAgents/scirag</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.CO, cs.AI</br><p>We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on
105 Cosmology Question-Answer (QA) pairs that we built specifically for this
purpose.The RAG configurations are manually evaluated by a human expert, that
is, a total of 945 generated answers were assessed. We find that currently the
best RAG agent configuration is with OpenAI embedding and generative model,
yielding 91.4\% accuracy. Using our human evaluation results we calibrate
LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human
evaluation. These results allow us to systematically select the best RAG agent
configuration for multi-agent system for autonomous scientific discovery in
astrophysics (e.g., cmbagent presented in a companion paper) and provide us
with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We
make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system
publicly available for further use by the astrophysics community.</p></br><a href="http://arxiv.org/pdf/2507.07257v1"><h2>Open Source Planning & Control System with Language Agents for
  Autonomous Scientific Discovery</h2></a>Authors:  Licong Xu, Milind Sarkar, Anto I. Lonappan, Íñigo Zubeldia, Pablo Villanueva-Domingo, Santiago Casas, Christian Fidler, Chetana Amancharla, Ujjwal Tiwari, Adrian Bayer, Chadi Ait Ekiou, Miles Cranmer, Adrian Dimitrov, James Fergusson, Kahaan Gandhi, Sven Krippendorf, Andrew Laverick, Julien Lesgourgues, Antony Lewis, Thomas Meier, Blake Sherwin, Kristen Surrao, Francisco Villaescusa-Navarro, Chi Wang, Xueqing Xu, Boris Bolliet</br>Comments: Accepted contribution to the ICML 2025 Workshop on Machine Learning
  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent; Videos:
  https://www.youtube.com/@cmbagent; HuggingFace:
  https://huggingface.co/spaces/astropilot-ai/cmbagent; Cloud:
  https://cmbagent.cloud</br>Primary Category: cs.AI</br>All Categories: cs.AI, astro-ph.IM, cs.CL, cs.MA</br><p>We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.</p></br><a href="http://arxiv.org/pdf/2507.05313v1"><h2>Solar Flare Prediction Using LSTM and DLSTM with Sliding Window Pattern
  Recognition</h2></a>Authors:  Zeinab Hassani, Davud Mohammadpur, Hossein Safari</br>Comments: Published in the Astrophysical Journal Supplement Series, volume 279,
  2025, DOI: 10.3847/1538-4365/addc73</br>Primary Category: astro-ph.SR</br>All Categories: astro-ph.SR, cs.AI, cs.LG</br><p>We investigate the use of Long Short-Term Memory (LSTM) and
Decomposition-LSTM (DLSTM) networks, combined with an ensemble algorithm, to
predict solar flare occurrences using time-series data from the GOES catalog.
The dataset spans from 2003 to 2023 and includes 151,071 flare events. Among
approximately possible patterns, 7,552 yearly pattern windows are identified,
highlighting the challenge of long-term forecasting due to the Sun's complex,
self-organized criticality-driven behavior. A sliding window technique is
employed to detect temporal quasi-patterns in both irregular and regularized
flare time series. Regularization reduces complexity, enhances large flare
activity, and captures active days more effectively. To address class
imbalance, resampling methods are applied. LSTM and DLSTM models are trained on
sequences of peak fluxes and waiting times from irregular time series, while
LSTM and DLSTM, integrated with an ensemble approach, are applied to sliding
windows of regularized time series with a 3-hour interval. Performance metrics,
particularly TSS (0.74), recall (0.95) and the area under the curve (AUC=0.87)
in the receiver operating characteristic (ROC), indicate that DLSTM with an
ensemble approach on regularized time series outperforms other models, offering
more accurate large-flare forecasts with fewer false errors compared to models
trained on irregular time series. The superior performance of DLSTM is
attributed to its ability to decompose time series into trend and seasonal
components, effectively isolating random noise. This study underscores the
potential of advanced machine learning techniques for solar flare prediction
and highlights the importance of incorporating various solar cycle phases and
resampling strategies to enhance forecasting reliability.</p></br>
