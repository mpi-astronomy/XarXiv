search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202411082000+TO+202411142000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on physics.data-an, stat.*, cs.LG, cs.AI staritng 202411082000 and ending 202411142000</h1>Feed last updated: 2024-11-13T00:00:00-05:00<a href="http://arxiv.org/pdf/2411.05960v1"><h2>A method based on Generative Adversarial Networks for disentangling
  physical and chemical properties of stars in astronomical spectra</h2></a>Authors:  Raúl Santoveña, Carlos Dafonte, Minia Manteiga</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.SR, cs.LG, 68T07, 85-08, 85-11</br><p>Data compression techniques focused on information preservation have become
essential in the modern era of big data. In this work, an encoder-decoder
architecture has been designed, where adversarial training, a modification of
the traditional autoencoder, is used in the context of astrophysical spectral
analysis. The goal of this proposal is to obtain an intermediate representation
of the astronomical stellar spectra, in which the contribution to the flux of a
star due to the most influential physical properties (its surface temperature
and gravity) disappears and the variance reflects only the effect of the
chemical composition over the spectrum. A scheme of deep learning is used with
the aim of unraveling in the latent space the desired parameters of the rest of
the information contained in the data. This work proposes a version of
adversarial training that makes use of a discriminator per parameter to be
disentangled, thus avoiding the exponential combination that occurs in the use
of a single discriminator, as a result of the discretization of the values to
be untangled. To test the effectiveness of the method, synthetic astronomical
data are used from the APOGEE and Gaia surveys. In conjunction with the work
presented, we also provide a disentangling framework (GANDALF) available to the
community, which allows the replication, visualization, and extension of the
method to domains of any nature.</p></br><a href="http://arxiv.org/pdf/2411.08842v1"><h2>AstroM$^3$: A self-supervised multimodal model for astronomy</h2></a>Authors:  Mariia Rizhko, Joshua S. Bloom</br>Comments: No comment found</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI</br><p>While machine-learned models are now routinely employed to facilitate
astronomical inquiry, model inputs tend to be limited to a primary data source
(namely images or time series) and, in the more advanced approaches, some
metadata. Yet with the growing use of wide-field, multiplexed observational
resources, individual sources of interest often have a broad range of
observational modes available. Here we construct an astronomical multimodal
dataset and propose AstroM$^3$, a self-supervised pre-training approach that
enables a model to learn from multiple modalities simultaneously. Specifically,
we extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodal
setting, allowing the integration of time-series photometry data, spectra, and
astrophysical metadata. In a fine-tuning supervised setting, our results
demonstrate that CLIP pre-training improves classification performance for
time-series photometry, where accuracy increases from 84.6% to 91.5%.
Furthermore, CLIP boosts classification accuracy by up to 12.6% when the
availability of labeled data is limited, showing the effectiveness of
leveraging larger corpora of unlabeled data. In addition to fine-tuned
classification, we can use the trained model in other downstream tasks that are
not explicitly contemplated during the construction of the self-supervised
model. In particular we show the efficacy of using the learned embeddings for
misclassifications identification, similarity search, and anomaly detection.
One surprising highlight is the "rediscovery" of Mira subtypes and two
Rotational variable subclasses using manifold learning and dimension reduction
algorithm. To our knowledge this is the first construction of an $n>2$ mode
model in astronomy. Extensions to $n>3$ modes is naturally anticipated with
this approach.</p></br>
