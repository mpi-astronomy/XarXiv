search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202405292000+TO+202406042000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, physics.data-an, stat.*, cs.AI staritng 202405292000 and ending 202406042000</h1>Feed last updated: 2024-06-04T00:00:00-04:00<a href="http://arxiv.org/pdf/2405.20590v1"><h2>Class-Based Time Series Data Augmentation to Mitigate Extreme Class
  Imbalance for Solar Flare Prediction</h2></a>Authors:  Junzhi Wen, Rafal A. Angryk</br>Comments: No comment found</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM, astro-ph.SR, cs.AI</br><p>Time series data plays a crucial role across various domains, making it
valuable for decision-making and predictive modeling. Machine learning (ML) and
deep learning (DL) have shown promise in this regard, yet their performance
hinges on data quality and quantity, often constrained by data scarcity and
class imbalance, particularly for rare events like solar flares. Data
augmentation techniques offer a potential solution to address these challenges,
yet their effectiveness on multivariate time series datasets remains
underexplored. In this study, we propose a novel data augmentation method for
time series data named Mean Gaussian Noise (MGN). We investigate the
performance of MGN compared to eight existing basic data augmentation methods
on a multivariate time series dataset for solar flare prediction, SWAN-SF,
using a ML algorithm for time series data, TimeSeriesSVC. The results
demonstrate the efficacy of MGN and highlight its potential for improving
classification performance in scenarios with extremely imbalanced data. Our
time complexity analysis shows that MGN also has a competitive computational
cost compared to the investigated alternative methods.</p></br><a href="http://arxiv.org/pdf/2405.20389v1"><h2>Designing an Evaluation Framework for Large Language Models in Astronomy
  Research</h2></a>Authors:  John F. Wu, Alina Hyk, Kiera McCormick, Christine Ye, Simone Astarita, Elina Baral, Jo Ciuca, Jesse Cranney, Anjalie Field, Kartheik Iyer, Philipp Koehn, Jenn Kotler, Sandor Kruk, Michelle Ntampaka, Charles O'Neill, Joshua E. G. Peek, Sanjib Sharma, Mikaeel Yunus</br>Comments: 7 pages, 3 figures. Code available at
  https://github.com/jsalt2024-evaluating-llms-for-astronomy/astro-arxiv-bot</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI, cs.HC, cs.IR</br><p>Large Language Models (LLMs) are shifting how scientific research is done. It
is imperative to understand how researchers interact with these models and how
scientific sub-communities like astronomy might benefit from them. However,
there is currently no standard for evaluating the use of LLMs in astronomy.
Therefore, we present the experimental design for an evaluation study on how
astronomy researchers interact with LLMs. We deploy a Slack chatbot that can
answer queries from users via Retrieval-Augmented Generation (RAG); these
responses are grounded in astronomy papers from arXiv. We record and anonymize
user questions and chatbot answers, user upvotes and downvotes to LLM
responses, user feedback to the LLM, and retrieved documents and similarity
scores with the query. Our data collection method will enable future dynamic
evaluations of LLM tools for astronomy.</p></br>
