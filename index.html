search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202511292000+TO+202512052000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, physics.data-an, cs.AI, cs.LG staritng 202511292000 and ending 202512052000</h1>Feed last updated: 2025-12-05T04:20:39Z<a href="https://arxiv.org/pdf/2512.01576v1"><h2>From Black Hole to Galaxy: Neural Operator: Framework for Accretion and Feedback Dynamics</h2></a>Authors:  Nihaal Bhojwani, Chuwei Wang, Hai-Yang Wang, Chang Sun, Elias R. Most, Anima Anandkumar</br>Comments: ML4PS Workshop, Neurips 2025 accepted</br>Primary Category: astro-ph.HE</br>All Categories: astro-ph.HE, astro-ph.GA, cs.AI, gr-qc</br><p>Modeling how supermassive black holes co-evolve with their host galaxies is notoriously hard because the relevant physics spans nine orders of magnitude in scale-from milliparsecs to megaparsecs--making end-to-end first-principles simulation infeasible. To characterize the feedback from the small scales, existing methods employ a static subgrid scheme or one based on theoretical guesses, which usually struggle to capture the time variability and derive physically faithful results. Neural operators are a class of machine learning models that achieve significant speed-up in simulating complex dynamics. We introduce a neural-operator-based ''subgrid black hole'' that learns the small-scale local dynamics and embeds it within the direct multi-level simulations. Trained on small-domain (general relativistic) magnetohydrodynamic data, the model predicts the unresolved dynamics needed to supply boundary conditions and fluxes at coarser levels across timesteps, enabling stable long-horizon rollouts without hand-crafted closures. Thanks to the great speedup in fine-scale evolution, our approach for the first time captures intrinsic variability in accretion-driven feedback, allowing dynamic coupling between the central black hole and galaxy-scale gas. This work reframes subgrid modeling in computational astrophysics with scale separation and provides a scalable path toward data-driven closures for a broad class of systems with central accretors.</p></br><a href="https://arxiv.org/pdf/2512.04803v1"><h2>287,872 Supermassive Black Holes Masses: Deep Learning Approaching Reverberation Mapping Accuracy</h2></a>Authors:  Yuhao Lu, HengJian SiTu, Jie Li, Yixuan Li, Yang Liu, Wenbin Lin, Yu Wang</br>Comments: 14 pages, 9 figures. Submitted to Journal of High Energy Astrophysics</br>Primary Category: astro-ph.GA</br>All Categories: astro-ph.GA, astro-ph.HE, astro-ph.IM, cs.AI</br><p>We present a population-scale catalogue of 287,872 supermassive black hole masses with high accuracy. Using a deep encoder-decoder network trained on optical spectra with reverberation-mapping (RM) based labels of 849 quasars and applied to all SDSS quasars up to $z=4$, our method achieves a root-mean-square error of $0.058$\,dex, a relative uncertainty of $\approx 14\%$, and coefficient of determination $R^{2}\approx0.91$ with respect to RM-based masses, far surpassing traditional single-line virial estimators. Notably, the high accuracy is maintained for both low ($<10^{7.5}\,M_\odot$) and high ($>10^{9}\,M_\odot$) mass quasars, where empirical relations are unreliable.</p></br><a href="https://arxiv.org/pdf/2512.04031v1"><h2>Large Language Models for Limited Noisy Data: A Gravitational Wave Identification Study</h2></a>Authors:  Yixuan Li, Yuhao Lu, Yang Liu, Liang Li, R. Ruffini, Di Li, Rong-Gen Cai, Xiaoyan Zhu, Wenbin Lin, Yu Wang</br>Comments: 10 pages, 5 figures</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.HE, cs.AI</br><p>This work investigates whether large language models (LLMs) offer advantages over traditional neural networks for astronomical data processing, in regimes with non-Gaussian, non-stationary noise and limited labeled samples. Gravitational wave observations provide an suitable test case, using only 90 LIGO events, finetuned LLMs achieve 97.4\% accuracy for identifying signals. Further experiments show that, in contrast to traditional networks that rely on large simulated datasets, additional simulated samples do not improve LLM performance, while scaling studies reveal predictable gains with increasing model size and dataset size. These results indicate that LLMs can extract discriminative structure directly from observational data and provide an efficient assessment for gravitational wave identification. The same strategy may extend to other astronomical domains with similar noise properties, such as radio or pulsar observations.</p></br><a href="https://arxiv.org/pdf/2512.04204v1"><h2>Machine Phenomenology: A Simple Equation Classifying Fast Radio Bursts</h2></a>Authors:  Yang Liu, Yuhao Lu, Rahim Moradi, Bo Yang, Bing Zhang, Wenbin Lin, Yu Wang</br>Comments: 19 pages, 9 figures, 3 tables. Submitted to SCIENCE CHINA Physics, Mechanics & Astronomy</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.HE, cs.AI</br><p>This work shows how human physical reasoning can guide machine-driven symbolic regression toward discovering empirical laws from observations. As an example, we derive a simple equation that classifies fast radio bursts (FRBs) into two distinct Gaussian distributions, indicating the existence of two physical classes. This human-AI workflow integrates feature selection, dimensional analysis, and symbolic regression: deep learning first analyzes CHIME Catalog 1 and identifies six independent parameters that collectively provide a complete description of FRBs; guided by Buckingham-$π$ analysis and correlation analysis, humans then construct dimensionless groups; finally, symbolic regression performed by the machine discovers the governing equation. When applied to the newer CHIME Catalog, the equation produces consistent results, demonstrating that it captures the underlying physics. This framework is applicable to a broad range of scientific domains.</p></br><a href="https://arxiv.org/pdf/2512.04600v1"><h2>The dynamical memory of tidal stellar streams: Joint inference of the Galactic potential and the progenitor of GD-1 with flow matching</h2></a>Authors:  Giuseppe Viterbo, Tobias Buck</br>Comments: submitted to A&A, comments welcome, all source code to reproduce this work can be found on GitHub under the url: https://github.com/vepe99/sbi-sim/tree/odisseo_branch . The simulator \textsc{\texttt{Odisseo}} is available on GitHub at the following url: https://github.com/vepe99/Odisseo</br>Primary Category: astro-ph.GA</br>All Categories: astro-ph.GA, physics.class-ph, physics.data-an, physics.space-ph</br><p>Stellar streams offer one of the most sensitive probes of the Milky Way`s gravitational potential, as their phase-space morphology encodes both the tidal field of the host galaxy and the internal structure of their progenitors. In this work, we introduce a framework that leverages Flow Matching and Simulation-Based Inference (SBI) to jointly infer the parameters of the GD-1 progenitor and the global properties of the Milky Way potential. Our aim is to move beyond traditional techniques (e.g. orbit-fitting and action-angle methods) by constructing a fully Bayesian, likelihood-free posterior over both host-galaxy parameters and progenitor properties, thereby capturing the intrinsic coupling between tidal stripping dynamics and the underlying potential. To achieve this, we generate a large suite of mock GD-1-like streams using our differentiable N-body code \textsc{\texttt{Odisseo}}, sampling self-consistent initial conditions from a Plummer sphere and evolving them in a flexible Milky Way potential model. We then apply conditional Flow Matching to learn the vector field that transports a base Gaussian distribution into the posterior, enabling efficient, amortized inference directly from stream phase-space data. We demonstrate that our method successfully recovers the true parameters of a fiducial GD-1 simulation, producing well-calibrated posteriors and accurately reproducing parameter degeneracies arising from progenitor-host interactions. Flow Matching provides a powerful, flexible framework for Galactic Archaeology. Our approach enables joint inference on progenitor and Galactic parameters, capturing complex dependencies that are difficult to model with classical likelihood-based methods.</p></br><a href="https://arxiv.org/pdf/2512.02968v1"><h2>Flexible Gravitational-Wave Parameter Estimation with Transformers</h2></a>Authors:  Annalena Kofler, Maximilian Dax, Stephen R. Green, Jonas Wildberger, Nihar Gupte, Jakob H. Macke, Jonathan Gair, Alessandra Buonanno, Bernhard Schölkopf</br>Comments: 8+11 pages, 3+7 figures</br>Primary Category: gr-qc</br>All Categories: gr-qc, astro-ph.IM, cs.LG</br><p>Gravitational-wave data analysis relies on accurate and efficient methods to extract physical information from noisy detector signals, yet the increasing rate and complexity of observations represent a growing challenge. Deep learning provides a powerful alternative to traditional inference, but existing neural models typically lack the flexibility to handle variations in data analysis settings. Such variations accommodate imperfect observations or are required for specialized tests, and could include changes in detector configurations, overall frequency ranges, or localized cuts. We introduce a flexible transformer-based architecture paired with a training strategy that enables adaptation to diverse analysis settings at inference time. Applied to parameter estimation, we demonstrate that a single flexible model -- called Dingo-T1 -- can (i) analyze 48 gravitational-wave events from the third LIGO-Virgo-KAGRA Observing Run under a wide range of analysis configurations, (ii) enable systematic studies of how detector and frequency configurations impact inferred posteriors, and (iii) perform inspiral-merger-ringdown consistency tests probing general relativity. Dingo-T1 also improves median sample efficiency on real events from a baseline of 1.4% to 4.2%. Our approach thus demonstrates flexible and scalable inference with a principled framework for handling missing or incomplete data -- key capabilities for current and next-generation observatories.</p></br><a href="https://arxiv.org/pdf/2512.00769v1"><h2>AI Agent for Source Finding by SoFiA-2 for SKA-SDC2</h2></a>Authors:  Xingchen Zhou, Nan Li, Peng Jia, Yingfeng Liu, Furen Deng, Shuanghao Shu, Ying Li, Liang Cao, Huanyuan Shan, Ayodeji Ibitoye</br>Comments: 20 pages, 10 figures, accepted by RAA</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.GA</br><p>Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.</p></br>
