search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202406182000+TO+202406242000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on cs.LG, cs.AI, stat.*, physics.data-an staritng 202406182000 and ending 202406242000</h1>Feed last updated: 2024-06-24T00:00:00-04:00<a href="http://arxiv.org/pdf/2406.14297v1"><h2>AI in Space for Scientific Missions: Strategies for Minimizing
  Neural-Network Model Upload</h2></a>Authors:  Jonah Ekelund, Ricardo Vinuesa, Yuri Khotyaintsev, Pierre Henri, Gian Luca Delzanno, Stefano Markidis</br>Comments: No comment found</br>Primary Category: cs.AI</br>All Categories: cs.AI, astro-ph.IM</br><p>Artificial Intelligence (AI) has the potential to revolutionize space
exploration by delegating several spacecraft decisions to an onboard AI instead
of relying on ground control and predefined procedures. It is likely that there
will be an AI/ML Processing Unit onboard the spacecraft running an inference
engine. The neural-network will have pre-installed parameters that can be
updated onboard by uploading, by telecommands, parameters obtained by training
on the ground. However, satellite uplinks have limited bandwidth and
transmissions can be costly. Furthermore, a mission operating with a suboptimal
neural network will miss out on valuable scientific data. Smaller networks can
thereby decrease the uplink cost, while increasing the value of the scientific
data that is downloaded. In this work, we evaluate and discuss the use of
reduced-precision and bare-minimum neural networks to reduce the time for
upload. As an example of an AI use case, we focus on the NASA's Magnetosperic
MultiScale (MMS) mission. We show how an AI onboard could be used in the
Earth's magnetosphere to classify data to selectively downlink higher value
data or to recognize a region-of-interest to trigger a burst-mode, collecting
data at a high-rate. Using a simple filtering scheme and algorithm, we show how
the start and end of a region-of-interest can be detected in on a stream of
classifications. To provide the classifications, we use an established
Convolutional Neural Network (CNN) trained to an accuracy >94%. We also show
how the network can be reduced to a single linear layer and trained to the same
accuracy as the established CNN. Thereby, reducing the overall size of the
model by up to 98.9%. We further show how each network can be reduced by up to
75% of its original size, by using lower-precision formats to represent the
network parameters, with a change in accuracy of less than 0.6 percentage
points.</p></br><a href="http://arxiv.org/pdf/2406.13638v1"><h2>XENONnT WIMP Search: Signal & Background Modeling and Statistical
  Inference</h2></a>Authors:  XENON Collaboration, E. Aprile, J. Aalbers, K. Abe, S. Ahmed Maouloud, L. Althueser, B. Andrieu, E. Angelino, D. Antón Martin, F. Arneodo, L. Baudis, M. Bazyk, L. Bellagamba, R. Biondi, A. Bismark, K. Boese, A. Brown, G. Bruno, R. Budnik, J. M. R. Cardoso, A. P. Cimental Chávez, A. P. Colijn, J. Conrad, J. J. Cuenca-García, V. D'Andrea, L. C. Daniel Garcia, M. P. Decowski, C. Di Donato, P. Di Gangi, S. Diglio, K. Eitel, A. Elykov, A. D. Ferella, C. Ferrari, H. Fischer, T. Flehmke, M. Flierman, W. Fulgione, C. Fuselli, P. Gaemers, R. Gaior, M. Galloway, F. Gao, S. Ghosh, R. Giacomobono, R. Glade-Beucke, L. Grandi, J. Grigat, H. Guan, M. Guida, P. Gyoergy, R. Hammann, A. Higuera, C. Hils, L. Hoetzsch, N. F. Hood, M. Iacovacci, Y. Itow, J. Jakob, F. Joerg, Y. Kaminaga, M. Kara, P. Kavrigin, S. Kazama, M. Kobayashi, A. Kopec, F. Kuger, H. Landsman, R. F. Lang, L. Levinson, I. Li, S. Li, S. Liang, Y. -T. Lin, S. Lindemann, M. Lindner, K. Liu, J. Loizeau, F. Lombardi, J. Long, J. A. M. Lopes, T. Luce, Y. Ma, C. Macolino, J. Mahlstedt, A. Mancuso, L. Manenti, F. Marignetti, T. Marrodán Undagoitia, K. Martens, J. Masbou, E. Masson, S. Mastroianni, A. Melchiorre, M. Messina, A. Michael, K. Miuchi, A. Molinario, S. Moriyama, K. Morå, Y. Mosbacher, M. Murra, J. Müller, K. Ni, U. Oberlack, B. Paetsch, Y. Pan, Q. Pellegrini, R. Peres, C. Peters, J. Pienaar, M. Pierre, G. Plante, T. R. Pollmann, L. Principe, J. Qi, J. Qin, D. Ramírez García, M. Rajado, R. Singh, L. Sanchez, J. M. F. dos Santos, I. Sarnoff, G. Sartorelli, J. Schreiner, D. Schulte, P. Schulte, H. Schulze Eißing, M. Schumann, L. Scotto Lavina, M. Selvi, F. Semeria, P. Shagin, S. Shi, J. Shi, M. Silva, H. Simgen, A. Takeda, P. -L. Tan, A. Terliuk, D. Thers, F. Toschi, G. Trinchero, C. D. Tunnell, F. Tönnies, K. Valerius, S. Vecchi, S. Vetter, F. I. Villazon Solar, G. Volta, C. Weinheimer, M. Weiss, D. Wenz, C. Wittweg, V. H. S. Wu, Y. Xing, D. Xu, Z. Xu, M. Yamashita, L. Yang, J. Ye, L. Yuan, G. Zavattini, M. Zhong</br>Comments: 20 pages, 10 figures</br>Primary Category: physics.data-an</br>All Categories: physics.data-an, astro-ph.IM, hep-ex, physics.ins-det</br><p>The XENONnT experiment searches for weakly-interacting massive particle
(WIMP) dark matter scattering off a xenon nucleus. In particular, XENONnT uses
a dual-phase time projection chamber with a 5.9-tonne liquid xenon target,
detecting both scintillation and ionization signals to reconstruct the energy,
position, and type of recoil. A blind search for nuclear recoil WIMPs with an
exposure of 1.1 tonne-years yielded no signal excess over background
expectations, from which competitive exclusion limits were derived on
WIMP-nucleon elastic scatter cross sections, for WIMP masses ranging from 6
GeV/$c^2$ up to the TeV/$c^2$ scale. This work details the modeling and
statistical methods employed in this search. By means of calibration data, we
model the detector response, which is then used to derive background and signal
models. The construction and validation of these models is discussed, alongside
additional purely data-driven backgrounds. We also describe the statistical
inference framework, including the definition of the likelihood function and
the construction of confidence intervals.</p></br>
