search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202506172000+TO+202506232000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on physics.data-an, cs.AI, stat.*, cs.LG staritng 202506172000 and ending 202506232000</h1>Feed last updated: 2025-06-23T00:00:00-04:00<a href="http://arxiv.org/pdf/2506.16255v1"><h2>Category-based Galaxy Image Generation via Diffusion Models</h2></a>Authors:  Xingzhong Fan, Hongming Tang, Yue Zeng, M. B. N. Kouwenhoven, Guangquan Zeng</br>Comments: 18 pages, 6 figures. Submitted to AAS Astronomical Journal (AJ) and
  is under revision. See another indenpdent work for furthur reference -- Can
  AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy
  Morphology Augmentation (Ma, Sun et al.). Comments are welcome</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, cs.AI</br><p>Conventional galaxy generation methods rely on semi-analytical models and
hydrodynamic simulations, which are highly dependent on physical assumptions
and parameter tuning. In contrast, data-driven generative models do not have
explicit physical parameters pre-determined, and instead learn them efficiently
from observational data, making them alternative solutions to galaxy
generation. Among these, diffusion models outperform Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs) in quality and diversity.
Leveraging physical prior knowledge to these models can further enhance their
capabilities. In this work, we present GalCatDiff, the first framework in
astronomy to leverage both galaxy image features and astrophysical properties
in the network design of diffusion models. GalCatDiff incorporates an enhanced
U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which
dynamically combines attention mechanisms with convolution operations to ensure
global consistency and local feature fidelity. Moreover, GalCatDiff uses
category embeddings for class-specific galaxy generation, avoiding the high
computational costs of training separate models for each category. Our
experimental results demonstrate that GalCatDiff significantly outperforms
existing methods in terms of the consistency of sample color and size
distributions, and the generated galaxies are both visually realistic and
physically consistent. This framework will enhance the reliability of galaxy
simulations and can potentially serve as a data augmentor to support future
galaxy classification algorithm development.</p></br><a href="http://arxiv.org/pdf/2506.16314v1"><h2>Signatures to help interpretability of anomalies</h2></a>Authors:  Emmanuel Gangler, Emille E. O. Ishida, Matwey V. Kornilov, Vladimir Korolev, Anastasia Lavrukhina, Konstantin Malanchev, Maria V. Pruzhinskaya, Etienne Russeil, Timofey Semenikhin, Sreevarsha Sreejith, Alina A. Volnova</br>Comments: 7 pages, 3 figure, proceedings of the International Conference on
  Machine Learning for Astrophysics (ML4ASTRO2)</br>Primary Category: cs.LG</br>All Categories: cs.LG, astro-ph.IM</br><p>Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.</p></br><a href="http://arxiv.org/pdf/2506.16233v1"><h2>Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy
  Morphology Augmentation</h2></a>Authors:  Chenrui Ma, Zechang Sun, Tao Jing, Zheng Cai, Yuan-Sen Ting, Song Huang, Mingyu Li</br>Comments: We have submitted to AAS journals. See another independent work for
  further reference -- Category-based Galaxy Image Generation via Diffusion
  Models (Fan, Tang et al.). Comments are welcome</br>Primary Category: astro-ph.GA</br>All Categories: astro-ph.GA, cs.LG</br><p>Observational astronomy relies on visual feature identification to detect
critical astrophysical phenomena. While machine learning (ML) increasingly
automates this process, models often struggle with generalization in
large-scale surveys due to the limited representativeness of labeled datasets
-- whether from simulations or human annotation -- a challenge pronounced for
rare yet scientifically valuable objects. To address this, we propose a
conditional diffusion model to synthesize realistic galaxy images for
augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains
visual feature -- galaxy image pairs from volunteer annotation, we demonstrate
that our model generates diverse, high-fidelity galaxy images closely adhere to
the specified morphological feature conditions. Moreover, this model enables
generative extrapolation to project well-annotated data into unseen domains and
advancing rare object detection. Integrating synthesized images into ML
pipelines improves performance in standard morphology classification, boosting
completeness and purity by up to 30\% across key metrics. For rare object
detection, using early-type galaxies with prominent dust lane features (
$\sim$0.1\% in GZ2 dataset) as a test case, our approach doubled the number of
detected instances from 352 to 872, compared to previous studies based on
visual inspection. This study highlights the power of generative models to
bridge gaps between scarce labeled data and the vast, uncharted parameter space
of observational astronomy and sheds insight for future astrophysical
foundation model developments. Our project homepage is available at
https://galaxysd-webpage.streamlit.app/.</p></br>
