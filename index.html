search_query=cat:astro-ph.*+AND+lastUpdatedDate:[202503222000+TO+202503282000]&start=0&max_results=5000
<h1>New astro-ph.* submissions cross listed on stat.*, cs.AI, physics.data-an, cs.LG staritng 202503222000 and ending 202503282000</h1>Feed last updated: 2025-03-28T00:00:00-04:00<a href="http://arxiv.org/pdf/2503.18617v1"><h2>Scaling Laws for Emulation of Stellar Spectra</h2></a>Authors:  Tomasz Różański, Yuan-Sen Ting</br>Comments: 25 pages, 11 figures, submitted to OJA</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, astro-ph.SR, cs.LG</br><p>Neural network-based emulators for the inference of stellar parameters and
elemental abundances represent an increasingly popular methodology in modern
spectroscopic surveys. However, these approaches are often constrained by their
emulation precision and domain transfer capabilities. Greater generalizability
has previously been achieved only with significantly larger model
architectures, as demonstrated by Transformer-based models in natural language
processing. This observation aligns with neural scaling laws, where model
performance predictably improves with increased model size, computational
resources allocated to model training, and training data volume. In this study,
we demonstrate that these scaling laws also apply to Transformer-based spectral
emulators in astronomy. Building upon our previous work with TransformerPayne
and incorporating Maximum Update Parametrization techniques from natural
language models, we provide training guidelines for scaling models to achieve
optimal performance. Our results show that within the explored parameter space,
clear scaling relationships emerge. These findings suggest that optimal
computational resource allocation requires balanced scaling. Specifically,
given a tenfold increase in training compute, achieving an optimal seven-fold
reduction in mean squared error necessitates an approximately 2.5-fold increase
in dataset size and a 3.8-fold increase in model size. This study establishes a
foundation for developing spectral foundational models with enhanced domain
transfer capabilities.</p></br><a href="http://arxiv.org/pdf/2503.21734v1"><h2>Structure and Melting of Fe, MgO, SiO2, and MgSiO3 in Planets: Database,
  Inversion, and Phase Diagram</h2></a>Authors:  Junjie Dong, Gabriel-Darius Mardaru, Paul D. Asimow, Lars P. Stixrude, Rebecca A. Fischer</br>Comments: Accepted for publication in The Planetary Science Journal on March
  27, 2025. 41 pages, 14 figures</br>Primary Category: astro-ph.EP</br>All Categories: astro-ph.EP, physics.data-an, physics.geo-ph</br><p>We present globally inverted pressure-temperature (P-T) phase diagrams up to
5,000 GPa for four fundamental planetary materials, Fe, MgO, SiO2, and MgSiO3,
derived from logistic regression and supervised learning, together with an
experimental phase equilibria database. These new P-T phase diagrams provide a
solution to long-standing disputes about their melting curves. Their
implications extend to the melting and freezing of rocky materials in the
interior of giant planets and super-Earth exoplanets, contributing to the
refinement of their internal structure models.</p></br><a href="http://arxiv.org/pdf/2503.18670v1"><h2>Deep learning-based identification of precipitation clouds from all-sky
  camera data for observatory safety</h2></a>Authors:  Mohammad H. Zhoolideh Haghighi, Alireza Ghasrimanesh, Habib Khosroshahi</br>Comments: A version of this work has been published in Machine Learning with
  Applications (MLWA)</br>Primary Category: astro-ph.IM</br>All Categories: astro-ph.IM, physics.data-an, stat.ML</br><p>For monitoring the night sky conditions, wide-angle all-sky cameras are used
in most astronomical observatories to monitor the sky cloudiness. In this
manuscript, we apply a deep-learning approach for automating the identification
of precipitation clouds in all-sky camera data as a cloud warning system. We
construct our original training and test sets using the all-sky camera image
archive of the Iranian National Observatory (INO). The training and test set
images are labeled manually based on their potential rainfall and their
distribution in the sky. We train our model on a set of roughly 2445 images
taken by the INO all-sky camera through the deep learning method based on the
EfficientNet network. Our model reaches an average accuracy of 99\% in
determining the cloud rainfall's potential and an accuracy of 96\% for cloud
coverage. To enable a comprehensive comparison and evaluate the performance of
alternative architectures for the task, we additionally trained three models
LeNet, DeiT, and AlexNet. This approach can be used for early warning of
incoming dangerous clouds toward telescopes and harnesses the power of deep
learning to automatically analyze vast amounts of all-sky camera data and
accurately identify precipitation clouds formations. Our trained model can be
deployed for real-time analysis, enabling the rapid identification of potential
threats, and offering a scaleable solution that can improve our ability to
safeguard telescopes and instruments in observatories. This is important now
that numerous small and medium-sized telescopes are increasingly integrated
with smart control systems to reduce manual operation.</p></br>
